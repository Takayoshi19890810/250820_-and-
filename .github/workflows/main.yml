name: Daily News Scraper & Aggregator

on:
  schedule:
    # 毎日 JST 15:10 に実行（UTC 06:10）
    - cron: '10 6 * * *'
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 25

    steps:
      - name: 📦 Check out repository
        uses: actions/checkout@v4

      - name: 🐍 Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: 🧰 Install Google Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y wget gnupg ca-certificates
          wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo dpkg -i google-chrome-stable_current_amd64.deb || sudo apt -f install -y
          google-chrome --version

      - name: 📦 Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ▶️ Run scraper & aggregator (summary sheet only)
        env:
          # 必須: サービスアカウントJSON（GitHub → Settings → Secrets and variables → Actions → Secrets）
          GCP_SERVICE_ACCOUNT_KEY: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
          # 任意: 検索キーワード（未設定時は '日産'）
          NEWS_KEYWORD: ${{ vars.NEWS_KEYWORD || '日産' }}
          # 最優先で正しい出力先シートIDを固定
          SPREADSHEET_ID: 1Vs4Cx8QPN4H2NOgtwaviOCe8zBTpUNDgJjqkHr51IZE
        run: |
          python main.py
