# -*- coding: utf-8 -*-
import os
import re
import json
import time
import unicodedata
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime

import gspread
from google.oauth2.service_account import Credentials
import google.generativeai as genai

# ========= Áí∞Â¢ÉÂ§âÊï∞ =========
NEWS_KEYWORD = os.getenv("NEWS_KEYWORD", "Êó•Áî£")
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
GCP_KEY = os.getenv("GCP_SERVICE_ACCOUNT_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ========= ÂÖ±ÈÄö =========
JST = timezone(timedelta(hours=9))
UA = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    )
}

def now_jst():
    return datetime.now(JST)

def fmt_jst(dt: datetime) -> str:
    return dt.astimezone(JST).strftime("%Y/%m/%d %H:%M")

def fetch_html(url: str, timeout: int = 20) -> str:
    try:
        r = requests.get(url, headers=UA, timeout=timeout)
        if r.ok:
            r.encoding = r.apparent_encoding or "utf-8"
            return r.text
    except Exception:
        pass
    return ""

# ========= Google Sheets Ë™çË®º =========
def get_spreadsheet(spreadsheet_id: str):
    if not GCP_KEY:
        raise RuntimeError("GCP_SERVICE_ACCOUNT_KEY „ÅåÊú™Ë®≠ÂÆö„Åß„Åô„ÄÇ")
    key_data = json.loads(GCP_KEY)
    creds = Credentials.from_service_account_info(
        key_data, scopes=["https://www.googleapis.com/auth/spreadsheets"]
    )
    client = gspread.authorize(creds)
    return client.open_by_key(spreadsheet_id)

# ========= Êó•‰ªò„Ç¶„Ç£„É≥„Éâ„Ç¶ÔºàÊò®Êó•15:00„Äú‰ªäÊó•14:59 / „Ç∑„Éº„ÉàÂêç=‰ªäÊó•„ÅÆYYMMDDÔºâ =========
def compute_window():
    now = now_jst()
    today_1500 = now.replace(hour=15, minute=0, second=0, microsecond=0)
    start = today_1500 - timedelta(days=1)          # Êò®Êó• 15:00
    end = today_1500                                 # ‰ªäÊó• 15:00ÔºàÊú™Ê∫ÄÂà§ÂÆöÔºâ
    sheet_name = now.strftime("%y%m%d")
    return start, end, sheet_name

def in_window_str(pub_str: str, start: datetime, end: datetime) -> bool:
    # pub_str „ÅØ "YYYY/MM/DD HH:MM"
    try:
        dt = datetime.strptime(pub_str, "%Y/%m/%d %H:%M").replace(tzinfo=JST)
        return start <= dt < end
    except Exception:
        return False

# ========= Ë®ò‰∫ã„Éö„Éº„Ç∏„Åã„ÇâÊó•ÊôÇ/„Çø„Ç§„Éà„É´/ÂºïÁî®ÂÖÉÊäΩÂá∫ =========
def try_parse_jst(dt_str: str):
    if not dt_str:
        return None
    patterns = [
        "%Y/%m/%d %H:%M", "%Y/%m/%d %H:%M:%S", "%Y/%m/%d",
        "%Y-%m-%d %H:%M", "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%dT%H:%M:%S.%fZ",
        "%Y-%m-%dT%H:%M:%S.%f%z",
    ]
    for p in patterns:
        try:
            d = datetime.strptime(dt_str, p)
            if p.endswith("Z"):
                d = d.replace(tzinfo=timezone.utc).astimezone(JST)
            elif "%z" in p:
                d = d.astimezone(JST)
            else:
                d = d.replace(tzinfo=JST)
            return d
        except Exception:
            pass
    return None

def extract_datetime_from_article(html: str) -> str:
    if not html:
        return ""
    soup = BeautifulSoup(html, "html.parser")
    # JSON-LD
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(tag.string or "{}")
            objs = data if isinstance(data, list) else [data]
            for obj in objs:
                if not isinstance(obj, dict):
                    continue
                for key in ("datePublished", "dateModified", "uploadDate"):
                    if obj.get(key):
                        dt = try_parse_jst(str(obj[key]).strip())
                        if dt:
                            return fmt_jst(dt)
        except Exception:
            continue
    # <time datetime="">
    t = soup.find("time", attrs={"datetime": True})
    if t and t.get("datetime"):
        dt = try_parse_jst(t["datetime"].strip())
        if dt:
            return fmt_jst(dt)
    # OGP/Meta
    for prop in ("article:published_time", "article:modified_time", "og:updated_time"):
        m = soup.find("meta", attrs={"property": prop, "content": True})
        if m and m.get("content"):
            dt = try_parse_jst(m["content"].strip())
            if dt:
                return fmt_jst(dt)
    return ""

def extract_yahoo_title_source(html: str) -> tuple[str, str]:
    title, source = "", "Yahoo!„Éã„É•„Éº„Çπ"
    if not html:
        return title, source
    soup = BeautifulSoup(html, "html.parser")
    # JSON-LD
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(tag.string or "{}")
            objs = data if isinstance(data, list) else [data]
            for obj in objs:
                if isinstance(obj, dict):
                    if not title and obj.get("headline"):
                        title = str(obj["headline"]).strip()
                    pub = obj.get("publisher")
                    if pub and isinstance(pub, dict) and pub.get("name"):
                        source = str(pub["name"]).strip() or source
        except Exception:
            continue
    # Fallback
    if not title:
        h1 = soup.find("h1")
        if h1:
            title = h1.get_text(strip=True)
    if not title:
        og = soup.find("meta", attrs={"property": "og:title", "content": True})
        if og and og.get("content"):
            t = og["content"].strip()
            if t and t != "Yahoo!„Éã„É•„Éº„Çπ":
                title = t
    return title, source

def resolve_yahoo_article_url(html: str, fallback_url: str) -> str:
    if not html:
        return fallback_url
    soup = BeautifulSoup(html, "html.parser")
    can = soup.find("link", rel="canonical")
    if can and can.get("href"):
        href = can["href"]
        if "news.yahoo.co.jp/articles/" in href:
            return href
    a = soup.select_one('a[href*="news.yahoo.co.jp/articles/"]')
    if a and a.get("href"):
        return a["href"]
    return fallback_url

# ========= „Éã„É•„Éº„ÇπÂèñÂæó =========
def fetch_google_news(keyword: str):
    url = f"https://news.google.com/rss/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    r = requests.get(url, headers=UA, timeout=20)
    r.raise_for_status()
    # XML„Éë„Éº„ÇµÂÑ™ÂÖà
    soup = None
    for parser in ("lxml-xml", "xml", "html.parser"):
        try:
            soup = BeautifulSoup(r.text, parser)
            if soup:
                break
        except Exception:
            soup = None
    if soup is None:
        return []
    items = []
    for it in soup.find_all("item"):
        title = (it.title.text if it.title else "").strip()
        link = (it.link.text if it.link else "").strip()
        source = (it.source.text if it.source else "Google„Éã„É•„Éº„Çπ").strip()
        pub = ""
        if it.pubDate and it.pubDate.text:
            try:
                dt = parsedate_to_datetime(it.pubDate.text.strip()).astimezone(JST)
                pub = fmt_jst(dt)
            except Exception:
                pub = ""
        if title and link:
            items.append(("Google", link, title, pub, source))
    return items

def fetch_msn_news(keyword: str):
    # Bing News RSS
    url = f"https://www.bing.com/news/search?q={keyword}&format=RSS&cc=JP"
    r = requests.get(url, headers=UA, timeout=20)
    r.raise_for_status()
    soup = None
    for parser in ("lxml-xml", "xml", "html.parser"):
        try:
            soup = BeautifulSoup(r.text, parser)
            if soup:
                break
        except Exception:
            soup = None
    if soup is None:
        return []
    items = []
    for it in soup.find_all("item"):
        title = (it.title.text if it.title else "").strip()
        link = (it.link.text if it.link else "").strip()
        source = (it.source.text if it.source else "MSN„Éã„É•„Éº„Çπ").strip()
        pub = ""
        if it.pubDate and it.pubDate.text:
            try:
                dt = parsedate_to_datetime(it.pubDate.text.strip()).astimezone(JST)
                pub = fmt_jst(dt)
            except Exception:
                pub = fmt_jst(now_jst())
        if title and link:
            items.append(("MSN", link, title, pub, source))
    return items

def fetch_yahoo_news(keyword: str):
    # ‚òÖ RSS„Åß„ÅØ„Å™„ÅèÊ§úÁ¥¢HTML„Åã„ÇâË®ò‰∫ãURL„ÇíÊäΩÂá∫ ‚Üí Ë®ò‰∫ã„Éö„Éº„Ç∏„ÅßÊó•ÊôÇ/„Çø„Ç§„Éà„É´/ÂºïÁî®ÂÖÉ„ÇíÂèñÂæó
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&ts=0&st=n&sr=1&sk=all"
    html = fetch_html(search_url)
    soup = BeautifulSoup(html, "html.parser")
    cand_urls = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "news.yahoo.co.jp/articles/" in href or "news.yahoo.co.jp/pickup/" in href:
            if href.startswith("//"):
                href = "https:" + href
            if href.startswith("http"):
                cand_urls.append(href)
    # ÈáçË§áÈô§Âéª
    seen, targets = set(), []
    for u in cand_urls:
        if u not in seen:
            seen.add(u)
            targets.append(u)

    items = []
    for u in targets:
        try:
            html0 = fetch_html(u)
            art_url = resolve_yahoo_article_url(html0, u)
            # pickup „ÅßË®ò‰∫ãURL„ÅåËß£Ê±∫„Åß„Åç„Å™„Åë„Çå„Å∞„Çπ„Ç≠„ÉÉ„Éó
            if "news.yahoo.co.jp/pickup/" in art_url and art_url == u:
                continue
            html1 = html0 if art_url == u else fetch_html(art_url)
            if not html1:
                continue

            title, source = extract_yahoo_title_source(html1)
            if not title or title == "Yahoo!„Éã„É•„Éº„Çπ":
                og = BeautifulSoup(html1, "html.parser").find("meta", attrs={"property": "og:title", "content": True})
                if og and og.get("content"):
                    t = og["content"].strip()
                    if t and t != "Yahoo!„Éã„É•„Éº„Çπ":
                        title = t

            pub = extract_datetime_from_article(html1) or fmt_jst(now_jst())

            items.append(("Yahoo", art_url, title, pub, source))
            time.sleep(0.2)
        except Exception:
            continue
    return items

# ========= „Ç´„ÉÜ„Ç¥„É™„ÉºÂà§ÂÆöÔºàÂº∑ÂåñÁâàÔºâ =========
def _norm(s: str) -> str:
    return unicodedata.normalize("NFKC", s or "")

BRAND_PATTERNS = [
    (re.compile(r"(Êó•Áî£|„Éã„ÉÉ„Çµ„É≥|NISSAN)", re.IGNORECASE), "„Éã„ÉÉ„Çµ„É≥"),
    (re.compile(r"(„Éà„É®„Çø|TOYOTA)", re.IGNORECASE), "„Éà„É®„Çø"),
    (re.compile(r"(„Éõ„É≥„ÉÄ|HONDA)", re.IGNORECASE), "„Éõ„É≥„ÉÄ"),
    (re.compile(r"(„Çπ„Éê„É´|SUBARU)", re.IGNORECASE), "„Çπ„Éê„É´"),
    (re.compile(r"(„Éû„ÉÑ„ÉÄ|MAZDA)", re.IGNORECASE), "„Éû„ÉÑ„ÉÄ"),
    (re.compile(r"(„Çπ„Ç∫„Ç≠|SUZUKI)", re.IGNORECASE), "„Çπ„Ç∫„Ç≠"),
    (re.compile(r"(‰∏âËè±|„Éü„ÉÑ„Éì„Ç∑|MITSUBISHI)", re.IGNORECASE), "„Éü„ÉÑ„Éì„Ç∑"),
    (re.compile(r"(„ÉÄ„Ç§„Éè„ÉÑ|DAIHATSU)", re.IGNORECASE), "„ÉÄ„Ç§„Éè„ÉÑ"),
]
NISSAN_MODELS = [
    "„É™„Éº„Éï","„Çª„É¨„Éä","„Çπ„Ç´„Ç§„É©„Ç§„É≥","„Éï„Çß„Ç¢„É¨„Éá„Ç£Z","„Éé„Éº„Éà","„Ç™„Éº„É©","„Ç¢„É™„Ç¢","„Ç≠„ÉÉ„ÇØ„Çπ",
    "„Ç®„ÇØ„Çπ„Éà„É¨„Ç§„É´","„Ç∏„É•„Éº„ÇØ","„Éá„Ç§„Ç∫","„É´„Éº„ÇØ„Çπ","„Éû„Éº„ÉÅ","„ÉÜ„Ç£„Ç¢„Éä","„Ç∑„É´„Éì„Ç¢","GT-R","„Çµ„ÇØ„É©","„Ç≠„É£„É©„Éê„É≥","„Éë„Éà„É≠„Éº„É´","„Éï„É≠„É≥„ÉÜ„Ç£„Ç¢"
]
RIVAL_BRANDS = {
    "„Éà„É®„Çø": ["„ÇØ„É©„Ç¶„É≥","„Éó„É™„Ç¶„Çπ","„Ç´„É≠„Éº„É©","„É§„É™„Çπ","„Ç¢„ÇØ„Ç¢","„Ç¢„É´„Éï„Ç°„Éº„Éâ","„É¥„Çß„É´„Éï„Ç°„Ç§„Ç¢","„Éè„É™„Ç¢„Éº","RAV4","GR86","„Çπ„Éº„Éó„É©","„É©„É≥„Éâ„ÇØ„É´„Éº„Ç∂„Éº"],
    "„Éõ„É≥„ÉÄ": ["„Ç∑„Éì„ÉÉ„ÇØ","„Éï„Ç£„ÉÉ„Éà","„É¥„Çß„Çº„É´","N-BOX","„Çπ„ÉÜ„ÉÉ„Éó„ÉØ„Ç¥„É≥","„Ç¢„Ç≥„Éº„Éâ","ZR-V","NSX","„Ç§„É≥„ÉÜ„Ç∞„É©"],
    "„Çπ„Éê„É´": ["„É¨„É¥„Ç©„Éº„Ç∞","„Éï„Ç©„É¨„Çπ„Çø„Éº","„Ç¢„Ç¶„Éà„Éê„ÉÉ„ÇØ","„Ç§„É≥„Éó„É¨„ÉÉ„Çµ","BRZ","„ÇΩ„É´„ÉÜ„É©"],
    "„Éû„ÉÑ„ÉÄ": ["„É≠„Éº„Éâ„Çπ„Çø„Éº","CX-5","CX-3","CX-30","MAZDA3","„Ç¢„ÉÜ„É≥„Ç∂","„Éá„Éü„Ç™","RX-7","RX-8"],
    "„Çπ„Ç∫„Ç≠": ["„Çπ„Ç§„Éï„Éà","„ÇΩ„É™„Ç™","„Éè„Çπ„É©„Éº","„Ç∏„É†„Éã„Éº","„Ç¢„É´„Éà","„ÉØ„Ç¥„É≥R","„Çπ„Éö„Éº„Ç∑„Ç¢"],
    "„Éü„ÉÑ„Éì„Ç∑": ["„Ç¢„Ç¶„Éà„É©„É≥„ÉÄ„Éº","„Éá„É™„Ç´","„Ç®„ÇØ„É™„Éó„Çπ„ÇØ„É≠„Çπ","RVR","„Éë„Ç∏„Çß„É≠"],
    "„ÉÄ„Ç§„Éè„ÉÑ": ["„Çø„É≥„Éà","„É†„Éº„É¥","„Éü„É©„Ç§„Éº„Çπ","„Çø„Éï„Éà","„É≠„ÉÉ„Ç≠„Éº","„Ç≥„Éö„É≥"]
}
GEN_PREFIXES = [("Êñ∞Âûã","Êñ∞Âûã"), ("ÁèæË°å","ÁèæË°å"), ("ÊóßÂûã","ÊóßÂûã"), ("ÂÖà‰ª£","ÊóßÂûã")]

TECH_KEYS = {
    "ev": ["EV","ÈõªÊ∞óËá™ÂãïËªä","BEV","„Éê„ÉÉ„ÉÜ„É™„Éº","ÊÄ•ÈÄüÂÖÖÈõª","ÂÖÖÈõªÁ∂≤","ÂÖÖÈõª„Çπ„Çø„É≥„Éâ","ÂÖÖÈõªÂô®","Ëà™Á∂öË∑ùÈõ¢","LFP","NCM"],
    "epower": ["e-POWER","ePOWER","„Ç§„Éº„Éë„ÉØ„Éº"],
    "e4orce": ["e-4ORCE","e4ORCE","4ORCE","4WD","AWD","2WD","ÂõõËº™ÈßÜÂãï"],
    "adas": ["Ëá™ÂãïÈÅãËª¢","„É¨„Éô„É´2","„É¨„Éô„É´3","ADAS","ÂÖàÈÄ≤ÈÅãËª¢ÊîØÊè¥","„Éó„É≠„Éë„Ç§„É≠„ÉÉ„Éà","ACC","„É¨„Éº„É≥„Ç≠„Éº„Éó","Ëá™ÂãïÈßêËªä"],
}
MOTORSPORT_KEYS = ["F1","„Éï„Ç©„Éº„Éü„É•„É©E","Formula E","WRC","„É©„É™„Éº","SUPER GT","„Çπ„Éº„Éë„ÉºGT","„É´„Éª„Éû„É≥","„É´„Éû„É≥","ËÄê‰πÖ„É¨„Éº„Çπ"]
COMPANY_KEYS = ["Ë≤©Â£≤Âè∞Êï∞","Ë≤©Â£≤", "ÁîüÁî£", "Â∑•Â†¥", "ÁîüÁî£ÂÅúÊ≠¢", "ÂÅúÊ≠¢", "Âá∫Ëç∑", "ÈõáÁî®", "‰∫∫Âì°", "„É™„Ç≥„Éº„É´", "ÊèêÊê∫", "Áµ±Âêà", "Âá∫Ë≥á", "ÊäïË≥á", "ÂÜçÂª∫", "Êí§ÈÄÄ", "„Çµ„Éó„É©„Ç§„É§„Éº", "ÂèóÊ≥®", "ËÉΩÂäõÂ¢óÂº∑"]
STOCK_KEYS = ["Ê†™","Ê†™‰æ°","‰∏äÂ†¥","IPO","Ëá™Á§æÊ†™Ë≤∑„ÅÑ","Ê±∫ÁÆó","ÈÄöÊúü","ÂõõÂçäÊúü","Â¢óÂèé","Ê∏õÁõä","‰∏äÊñπ‰øÆÊ≠£","‰∏ãÊñπ‰øÆÊ≠£","Ê•≠Á∏æ","Ë¶ãÈÄö„Åó"]
POLICY_KEYS = ["È¶ñÁõ∏","ÂÜÖÈñ£","Â§ßËá£","ÈÅ∏Êåô","Á®é","‰∫àÁÆó","Ë¶èÂà∂","Ë£úÂä©Èáë","Èñ¢Á®é","Êó•ÈäÄ","ÊôØÊ∞ó","ÁµåÊ∏àÂØæÁ≠ñ","ÁÇ∫Êõø","„Ç§„É≥„Éï„É¨","GDP","Ë≤°Êîø"]
SPORTS_KEYS = ["ÈáéÁêÉ","„Çµ„ÉÉ„Ç´„Éº","J„É™„Éº„Ç∞","MLB","WÊùØ","„ÉØ„Éº„É´„Éâ„Ç´„ÉÉ„Éó","„Éê„É¨„Éº„Éú„Éº„É´","„Éê„Çπ„Ç±„ÉÉ„Éà","NBA","È´òÊ†°ÈáéÁêÉ"]

def contains_any(t: str, keys: list[str]) -> bool:
    T = _norm(t)
    return any(k in T for k in keys)

def detect_brand_name(title: str) -> str|None:
    for pat, name in BRAND_PATTERNS:
        if pat.search(title):
            return name
    return None

def detect_nissan_model(title: str) -> str|None:
    T = _norm(title)
    for m in NISSAN_MODELS:
        if _norm(m) in T:
            return m
    return None

def detect_rival_model(title: str) -> bool:
    T = _norm(title)
    for brand, models in RIVAL_BRANDS.items():
        if brand in T and any(_norm(m) in T for m in models):
            return True
    modelish = any(k in T for k in ["Êñ∞Âûã","„É¢„Éá„É´","„Ç∞„É¨„Éº„Éâ","Áô∫Ë°®","Áô∫Â£≤","SUV","„Çª„ÉÄ„É≥","„Éè„ÉÉ„ÉÅ„Éê„ÉÉ„ÇØ","„ÇØ„Éº„Éö","„Éü„Éã„Éê„É≥"])
    return modelish and not ("Êó•Áî£" in T or "„Éã„ÉÉ„Çµ„É≥" in T or "NISSAN" in T)

def build_car_category(title: str) -> str|None:
    model = detect_nissan_model(title)
    if model:
        prefix = ""
        for key, norm in GEN_PREFIXES:
            if key in title:
                prefix = norm
                break
        label = f"{prefix}{model}" if prefix else model
        return f"ËªäÔºà{label}Ôºâ"
    if detect_rival_model(title):
        return "ËªäÔºàÁ´∂ÂêàÔºâ"
    return None

def build_company_category(title: str) -> str:
    brand = detect_brand_name(title)
    return f"‰ºöÁ§æÔºà{brand if brand else '„Åù„ÅÆ‰ªñ'}Ôºâ"

def normalize_category(title: str, gemini_cat: str) -> str:
    t = _norm(title)
    base = (gemini_cat or "").strip()

    # ÊäÄË°ìÁ¥∞ÁõÆ„ÇíÊúÄÂÑ™ÂÖà
    if contains_any(t, TECH_KEYS["epower"]):
        return "ÊäÄË°ìÔºàe-POWERÔºâ"
    if contains_any(t, TECH_KEYS["e4orce"]):
        return "ÊäÄË°ìÔºàe-4ORCEÔºâ"
    if contains_any(t, TECH_KEYS["adas"]):
        return "ÊäÄË°ìÔºàAD/ADASÔºâ"
    if contains_any(t, TECH_KEYS["ev"]):
        if contains_any(t, COMPANY_KEYS):
            return build_company_category(title)
        return "ÊäÄË°ìÔºàEVÔºâ"

    if contains_any(t, MOTORSPORT_KEYS):
        return "„É¢„Éº„Çø„Éº„Çπ„Éù„Éº„ÉÑ"
    if contains_any(t, STOCK_KEYS):
        return "Ê†™Âºè"
    if contains_any(t, POLICY_KEYS):
        return "ÊîøÊ≤ª„ÉªÁµåÊ∏à"
    if contains_any(t, SPORTS_KEYS):
        return "„Çπ„Éù„Éº„ÉÑ"

    car_cat = build_car_category(title)
    if car_cat:
        return car_cat

    if contains_any(t, COMPANY_KEYS) or detect_brand_name(title):
        return build_company_category(title)

    if base.startswith("Ëªä"):
        car_cat = build_car_category(title)
        if car_cat:
            return car_cat
        return "ËªäÔºàÁ´∂ÂêàÔºâ"
    if base in ["‰ºöÁ§æ","‰ºÅÊ•≠"]:
        return build_company_category(title)

    return "„Åù„ÅÆ‰ªñ"

# ========= „Éù„Ç∏„Éç„Ç¨ÔºàGeminiÔºã„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ =========
def _heuristic_sentiment(title: str) -> str:
    neg_kw = ["ÂÅúÊ≠¢","ÁµÇ‰∫Ü","Êí§ÈÄÄ","‰∏çÁ••‰∫ã","‰∏ãËêΩ","Âê¶ÂÆö","ÁÇé‰∏ä","‰∫ãÊïÖ","ÂïèÈ°å","Á†¥Ë´á","‰∫∫Âì°ÂâäÊ∏õ","ÈõáÁî®‰∏çÂÆâ","ÈñâÈéñ","Ê∏õÁî£"]
    pos_kw = ["Áô∫Ë°®","ÂèóË≥û","Â•ΩË™ø","‰∏äÊòá","ÁôªÂ†¥","ÂÖ¨Èñã","Êñ∞Âûã","Âº∑Âåñ","ÂèóÊ≥®","Áô∫Â£≤","„É©„Ç§„É≥„Éä„ÉÉ„Éó","Â¢óÂä†"]
    if any(k in title for k in neg_kw):
        return "„Éç„Ç¨„ÉÜ„Ç£„Éñ"
    if any(k in title for k in pos_kw):
        return "„Éù„Ç∏„ÉÜ„Ç£„Éñ"
    return "„Éã„É•„Éº„Éà„É©„É´"

def classify_titles_gemini_batched(titles: list[str], batch_size: int = 80) -> list[tuple[str, str]]:
    if not titles:
        return []
    if not GEMINI_API_KEY:
        out = []
        for t in titles:
            s = _heuristic_sentiment(t)
            c = normalize_category(t, "„Åù„ÅÆ‰ªñ")
            out.append((s, c))
        return out

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(
        "gemini-1.5-flash",
        generation_config={"response_mime_type": "application/json"}
    )

    out = [("", "")] * len(titles)
    for start_idx in range(0, len(titles), batch_size):
        batch = titles[start_idx:start_idx+batch_size]
        payload = [{"row": start_idx+i, "title": t} for i, t in enumerate(batch)]
        sys_prompt = (
            "„ÅÇ„Å™„Åü„ÅØÊïèËÖïÈõëË™åË®òËÄÖ„Åß„Åô„ÄÇÂêÑ„Çø„Ç§„Éà„É´„Å´„Å§„ÅÑ„Å¶‰ª•‰∏ã„ÇíÂà§ÂÆö„Åó„ÄÅ"
            "JSONÈÖçÂàó„ÅÆ„Åø„ÅßËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂêÑË¶ÅÁ¥†„ÅØ "
            '{"row": Êï∞ÂÄ§, "sentiment": "„Éù„Ç∏„ÉÜ„Ç£„Éñ|„Éç„Ç¨„ÉÜ„Ç£„Éñ|„Éã„É•„Éº„Éà„É©„É´", '
            '"category": "‰ºöÁ§æ|Ëªä|ËªäÔºàÁ´∂ÂêàÔºâ|ÊäÄË°ìÔºàEVÔºâ|ÊäÄË°ìÔºàe-POWERÔºâ|ÊäÄË°ìÔºàe-4ORCEÔºâ|'
            'ÊäÄË°ìÔºàAD/ADASÔºâ|ÊäÄË°ì|„É¢„Éº„Çø„Éº„Çπ„Éù„Éº„ÉÑ|Ê†™Âºè|ÊîøÊ≤ª„ÉªÁµåÊ∏à|„Çπ„Éù„Éº„ÉÑ|„Åù„ÅÆ‰ªñ"}„ÄÇ'
            "„Çø„Ç§„Éà„É´„ÅØÊîπÂ§â„Åó„Å™„ÅÑ„ÄÇ„Ç´„ÉÜ„Ç¥„É™„ÅØÂçò‰∏Ä„ÄÇ"
        )
        try:
            resp = model.generate_content([
                sys_prompt,
                {"mime_type": "application/json", "text": json.dumps(payload, ensure_ascii=False)}
            ])
            text = (getattr(resp, "text", "") or "").strip()
            arr = None
            try:
                if text:
                    arr = json.loads(text)
            except Exception:
                # „Ç¨„Éº„ÉâÔºöÊú¨Êñá„Å´ÂâçÂæåË™¨Êòé„ÅåÊ∑∑„Åñ„Å£„ÅüÊôÇ
                s = text.find("[")
                e = text.rfind("]")
                if s != -1 and e != -1 and e > s:
                    try:
                        arr = json.loads(text[s:e+1])
                    except Exception:
                        arr = None
            if isinstance(arr, dict):
                arr = [arr]
            if isinstance(arr, list):
                for obj in arr:
                    try:
                        idx = int(obj.get("row"))
                        s = str(obj.get("sentiment", "")).strip()
                        c = str(obj.get("category", "")).strip()
                        if 0 <= idx < len(out):
                            out[idx] = (s, c)
                    except Exception:
                        continue
            # Ê≠£Ë¶èÂåñ
            for i in range(start_idx, start_idx+len(batch)):
                s, c = out[i]
                s = s or _heuristic_sentiment(titles[i])
                c = normalize_category(titles[i], c or "„Åù„ÅÆ‰ªñ")
                out[i] = (s, c)
        except Exception as e:
            print(f"Gemini„Éê„ÉÉ„ÉÅÂ§±Êïó: {e}")
            for i in range(start_idx, start_idx+len(batch)):
                s = _heuristic_sentiment(titles[i])
                c = normalize_category(titles[i], "„Åù„ÅÆ‰ªñ")
                out[i] = (s, c)
        time.sleep(0.2)
    return out

# ========= ÈõÜÁ¥Ñ & Âá∫Âäõ =========
def build_daily_sheet(sh, msn_items, google_items, yahoo_items):
    start, end, sheet_name = compute_window()

    msn_f    = [x for x in msn_items    if x[3] and in_window_str(x[3], start, end)]
    google_f = [x for x in google_items if x[3] and in_window_str(x[3], start, end)]
    yahoo_f  = [x for x in yahoo_items  if x[3] and in_window_str(x[3], start, end)]

    print(f"üìä „Éï„Ç£„É´„ÇøÁµêÊûú: MSN={len(msn_f)}, Google={len(google_f)}, Yahoo={len(yahoo_f)}")
    ordered = msn_f + google_f + yahoo_f  # ‰∏¶„Å≥ÔºöMSN‚ÜíGoogle‚ÜíYahoo

    # „Çø„Ç§„Éà„É´‰∏ÄÊã¨ÂàÜÈ°û
    titles = [row[2] for row in ordered]
    senti_cate = classify_titles_gemini_batched(titles)

    # „Ç∑„Éº„Éà
    try:
        ws = sh.worksheet(sheet_name)
        ws.clear()
    except Exception:
        ws = sh.add_worksheet(title=sheet_name, rows="5000", cols="10")

    headers = ["„ÇΩ„Éº„Çπ", "URL", "„Çø„Ç§„Éà„É´", "ÊäïÁ®øÊó•", "ÂºïÁî®ÂÖÉ", "„Ç≥„É°„É≥„ÉàÊï∞", "„Éù„Ç∏„Éç„Ç¨", "„Ç´„ÉÜ„Ç¥„É™"]
    ws.update("A1:H1", [headers])

    rows = []
    for i, row in enumerate(ordered):
        src, url, title, pub, origin = row
        sentiment, category = senti_cate[i] if i < len(senti_cate) else ("", "")
        rows.append([src, url, title, pub, origin, "", sentiment, category])

    if rows:
        ws.update(f"A2:H{len(rows)+1}", rows)

    print(f"üïí ÈõÜÁ¥ÑÊúüÈñì: {start.strftime('%Y/%m/%d %H:%M')} „Äú {(end - timedelta(minutes=1)).strftime('%Y/%m/%d %H:%M')} ‚Üí „Ç∑„Éº„ÉàÂêç: {sheet_name}")
    print(f"‚úÖ ÈõÜÁ¥Ñ„Ç∑„Éº„Éà {sheet_name}: {len(rows)} ‰ª∂")
    return sheet_name

# ========= „É°„Ç§„É≥ =========
def main():
    print(f"üîé „Ç≠„Éº„ÉØ„Éº„Éâ: {NEWS_KEYWORD}")
    print(f"üìÑ SPREADSHEET_ID: {SPREADSHEET_ID}")
    if not SPREADSHEET_ID:
        raise RuntimeError("SPREADSHEET_ID „ÅåÊú™Ë®≠ÂÆö„Åß„Åô„ÄÇ")
    sh = get_spreadsheet(SPREADSHEET_ID)
    print(f"üìò Opened spreadsheet title: {sh.title}")

    print("\n--- ÂèñÂæó ---")
    google_items = fetch_google_news(NEWS_KEYWORD)
    yahoo_items  = fetch_yahoo_news(NEWS_KEYWORD)   # ‚Üê 404ÂØæÁ≠ñÁâàÔºàHTML‚ÜíË®ò‰∫ãËß£ÊûêÔºâ
    msn_items    = fetch_msn_news(NEWS_KEYWORD)

    print(f"‚úÖ Google„Éã„É•„Éº„Çπ: {len(google_items)} ‰ª∂ÔºàÊäïÁ®øÊó•ÂèñÂæó {sum(1 for i in google_items if i[3])} ‰ª∂Ôºâ")
    print(f"‚úÖ Yahoo!„Éã„É•„Éº„Çπ: {len(yahoo_items)} ‰ª∂ÔºàÊäïÁ®øÊó•ÂèñÂæó {sum(1 for i in yahoo_items if i[3])} ‰ª∂Ôºâ")
    print(f"‚úÖ MSN„Éã„É•„Éº„Çπ: {len(msn_items)} ‰ª∂ÔºàÊäïÁ®øÊó•ÂèñÂæó/Êé®ÂÆö {sum(1 for i in msn_items if i[3])} ‰ª∂Ôºâ")

    print("\n--- ÈõÜÁ¥ÑÔºà„Åæ„Å®„ÇÅ„Ç∑„Éº„Éà„ÅÆ„Åø / AÂàó=„ÇΩ„Éº„Çπ / È†Ü=MSN‚ÜíGoogle‚ÜíYahooÔºâ ---")
    build_daily_sheet(sh, msn_items, google_items, yahoo_items)

if __name__ == "__main__":
    main()
