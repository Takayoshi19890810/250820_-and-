# -*- coding: utf-8 -*-
import os
import re
import json
import time
import random
import traceback
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime

import requests
from bs4 import BeautifulSoup

import gspread
from google.oauth2.service_account import Credentials

# ========= åŸºæœ¬è¨­å®š =========
JST = timezone(timedelta(hours=9))
UA = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120 Safari/537.36"}

KEYWORD = os.getenv("KEYWORD", os.getenv("NEWS_KEYWORD", "æ—¥ç”£")).strip()
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID", "").strip()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")

USE_WDM = int(os.getenv("USE_WDM", "0"))   # 0: Selenium Manager, 1: webdriver-manager
SCROLL_SLEEP = float(os.getenv("SCROLL_SLEEP", "1.2"))
SCROLLS_GOOGLE = int(os.getenv("SCROLLS_GOOGLE", "4"))
SCROLLS_YAHOO = int(os.getenv("SCROLLS_YAHOO", "4"))
ALLOW_PICKUP_FALLBACK = int(os.getenv("ALLOW_PICKUP_FALLBACK", "1"))

# ========= å…±é€šãƒ˜ãƒ«ãƒ‘ =========
def soup(html: str):
    try:
        return BeautifulSoup(html, "lxml")
    except Exception:
        return BeautifulSoup(html, "html.parser")

def fmt_jst(dt: datetime) -> str:
    return dt.astimezone(JST).strftime("%Y/%m/%d %H:%M")

def parse_last_modified(url: str) -> str:
    try:
        r = requests.head(url, headers=UA, timeout=10, allow_redirects=True)
        lm = r.headers.get("Last-Modified")
        if lm:
            dt = parsedate_to_datetime(lm).astimezone(JST)
            return fmt_jst(dt)
    except Exception:
        pass
    return ""

def fetch_html(url: str, timeout=15) -> str:
    try:
        r = requests.get(url, headers=UA, timeout=timeout)
        r.raise_for_status()
        return r.text
    except Exception:
        return ""

# ========= æœŸé–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆå‰æ—¥15:00ã€œå½“æ—¥14:59ï¼‰ =========
def compute_window_and_sheet_name(now: datetime):
    today = now.astimezone(JST).date()
    start = datetime.combine(today - timedelta(days=1), datetime.min.time()).replace(tzinfo=JST) + timedelta(hours=15)
    end = datetime.combine(today, datetime.min.time()).replace(tzinfo=JST) + timedelta(hours=14, minutes=59, seconds=59)
    sheet_name = now.astimezone(JST).strftime("%y%m%d")
    return start, end, sheet_name

def in_window(pub_str: str, start: datetime, end: datetime) -> bool:
    try:
        dt = datetime.strptime(pub_str, "%Y/%m/%d %H:%M").replace(tzinfo=JST)
        return start <= dt <= end
    except Exception:
        return False

# ========= Selenium =========
def get_driver():
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1280,2000")
    options.add_argument("--lang=ja-JP")
    if USE_WDM:
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
    else:
        driver = webdriver.Chrome(options=options)  # Selenium Manager
    return driver

def smooth_scroll(driver, times=4, sleep=1.2):
    last_h = driver.execute_script("return document.body.scrollHeight")
    for _ in range(times):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(sleep)
        new_h = driver.execute_script("return document.body.scrollHeight")
        if new_h == last_h:
            break
        last_h = new_h

# ========= MSN =========
def fetch_msn(keyword: str):
    items = []
    try:
        driver = get_driver()
        url = f"https://www.bing.com/news/search?q={keyword}&qft=sortbydate%3d'1'&form=YFNR"
        driver.get(url)
        time.sleep(3)
        smooth_scroll(driver, times=2, sleep=SCROLL_SLEEP)
        sp = soup(driver.page_source)
        driver.quit()
        cards = sp.select("div.news-card")
        for c in cards:
            title = c.get("data-title", "").strip()
            url = c.get("data-url", "").strip()
            source = c.get("data-author", "").strip() or "MSN"
            pub = parse_last_modified(url) if url else ""
            if title and url and pub:
                items.append(("MSN", url, title, pub, source))
    except Exception:
        traceback.print_exc()
    return items

# ========= Google =========
def fetch_google(keyword: str):
    items = []
    driver = None
    try:
        driver = get_driver()
        url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
        driver.get(url)
        time.sleep(3)
        smooth_scroll(driver, times=SCROLLS_GOOGLE, sleep=SCROLL_SLEEP)
        sp = soup(driver.page_source)
        driver.quit()
        seen = set()
        for a in sp.find_all("a", href=True):
            href = a["href"]
            if "/articles/" not in href:
                continue
            full = "https://news.google.com" + href[1:] if href.startswith("./") else ("https://news.google.com"+href if href.startswith("/") else href)
            title = a.get_text(strip=True)
            try:
                final = requests.get(full, headers=UA, timeout=10, allow_redirects=True).url
            except Exception:
                final = full
            pub = parse_last_modified(final)
            if final not in seen and title and pub:
                seen.add(final)
                items.append(("Google", final, title, pub, ""))
    except Exception:
        traceback.print_exc()
    finally:
        if driver:
            try: driver.quit()
            except: pass
    return items

# ========= Yahoo =========
def fetch_yahoo(keyword: str):
    items = []
    driver = None
    try:
        driver = get_driver()
        url = (
            "https://news.yahoo.co.jp/search"
            f"?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
        )
        driver.get(url)
        time.sleep(3)
        smooth_scroll(driver, times=SCROLLS_YAHOO, sleep=SCROLL_SLEEP)
        sp = soup(driver.page_source)
        driver.quit()
        cand = []
        for a in sp.find_all("a", href=True):
            href = a["href"]
            if not href.startswith("http"):
                if href.startswith("//"):
                    href = "https:" + href
                elif href.startswith("/"):
                    href = "https://news.yahoo.co.jp" + href
            if "news.yahoo.co.jp/articles/" in href or "news.yahoo.co.jp/pickup/" in href:
                cand.append(href)
        seen = set()
        for u in cand:
            if u in seen: continue
            seen.add(u)
            html = fetch_html(u)
            sp1 = soup(html)
            h1 = sp1.find("h1")
            title = h1.get_text(strip=True) if h1 else ""
            pub = parse_last_modified(u)
            if (title or (ALLOW_PICKUP_FALLBACK and u)) and pub:
                items.append(("Yahoo", u, title or "", pub, ""))
    except Exception:
        traceback.print_exc()
    finally:
        if driver:
            try: driver.quit()
            except: pass
    return items

# ========= Gemini åˆ†é¡ï¼ˆç•ªå·+ã‚¿ã‚¤ãƒˆãƒ«â†’4åˆ—TSVã§è¿”ã•ã›ã‚‹ï¼‰ =========
GEMINI_PROMPT = """ã‚ãªãŸã¯æ•è…•é›‘èªŒè¨˜è€…ã§ã™ã€‚ ä¸Šè¨˜Webãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä»¥ä¸‹ã®è¦–ç‚¹ã§åˆ¤æ–­ã—ã¦ã»ã—ã„ã€‚
â‘ ãƒã‚¸ãƒ†ã‚£ãƒ–ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã®åˆ¤åˆ¥ã€‚
â‘¡è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®åˆ¤åˆ¥ã€‚ã€€ä»¥ä¸‹ã«ä¾‹ã‚’è¨˜è¼‰ã—ã¦ã»ã—ã„ã€‚
ä¼šç¤¾ï¼šä¼æ¥­ã®æ–½ç­–ã‚„ç”Ÿç”£ã€è²©å£²å°æ•°ãªã©ã€‚ã€€ãƒ‹ãƒƒã‚µãƒ³ã€ãƒˆãƒ¨ã‚¿ã€ãƒ›ãƒ³ãƒ€ã€ã‚¹ãƒãƒ«ã€ãƒãƒ„ãƒ€ã€ã‚¹ã‚ºã‚­ã€ãƒŸãƒ„ãƒ“ã‚·ã€ãƒ€ã‚¤ãƒãƒ„ã®è¨˜äº‹ã®å ´åˆã€()ä»˜ã§ä¼æ¥­åã‚’æ›¸ã„ã¦ã€‚ãã‚Œä»¥å¤–ã¯ãã®ä»–ã€‚
è»Šï¼šã‚¯ãƒ«ãƒã®åç§°ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‚‚ã®ï¼ˆä¼šç¤¾åã ã‘ã®å ´åˆã¯è»Šã«åˆ†é¡ã—ãªã„ï¼‰
æ–°å‹/ç¾è¡Œ/æ—§å‹+åç§°ã‚’()ä»˜ã§è¨˜è¼‰ã—ã¦ã€‚ï¼ˆä¾‹ãƒ»ãƒ»æ–°å‹ãƒªãƒ¼ãƒ•ã€ç¾è¡Œã‚»ãƒ¬ãƒŠã€æ—§å‹ã‚¹ã‚«ã‚¤ãƒ©ã‚¤ãƒ³ï¼‰
æ—¥ç”£ä»¥å¤–ã®è»Šã®å ´åˆã¯ã€è»Šï¼ˆç«¶åˆï¼‰ã¨è¨˜è¼‰ã—ã¦ã€‚
æŠ€è¡“ï¼ˆEVï¼‰ï¼šé›»æ°—è‡ªå‹•è»Šã®æŠ€è¡“ã«é–¢ã‚ã‚‹ã‚‚ã®ï¼ˆãƒãƒƒãƒ†ãƒªãƒ¼å·¥å ´å»ºè¨­ã‚„ä¼æ¥­ã®æ–½ç­–ã¯å«ã¾ãªã„ï¼‰
æŠ€è¡“ï¼ˆe-POWERï¼‰ï¼še-POWERã«é–¢ã‚ã‚‹ã‚‚ã®
æŠ€è¡“ï¼ˆe-4ORCEï¼‰ï¼š4WDã‚„2WDã€AWDã«é–¢ã‚ã‚‹ã‚‚ã®
æŠ€è¡“ï¼ˆAD/ADASï¼‰ï¼šè‡ªå‹•é‹è»¢ã‚„å…ˆé€²é‹è»¢ã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã‚ã‚‹ã‚‚ã®
æŠ€è¡“ï¼šä¸Šè¨˜ä»¥å¤–ã®æŠ€è¡“ã«é–¢ã‚ã‚‹ã‚‚ã®
ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚¹ãƒãƒ¼ãƒ„ï¼šF1ã‚„ãƒ©ãƒªãƒ¼ã€ãƒ•ã‚©ãƒŸãƒ¥ãƒ©ãƒ¼Eãªã©ã€è‡ªå‹•è»Šã®ãƒ¬ãƒ¼ã‚¹ã«é–¢ã‚ã‚‹ã‚‚ã®
æ ªå¼ï¼šæ ªå¼ç™ºè¡Œã‚„æ ªä¾¡ã®å€¤å‹•ãã€æŠ•è³‡ã«é–¢ã‚ã‚‹ã‚‚ã®
æ”¿æ²»ãƒ»çµŒæ¸ˆï¼šæ”¿æ²»å®¶ã‚„é¸æŒ™ã€ç¨é‡‘ã€çµŒæ¸ˆã«é–¢ã‚ã‚‹ã‚‚ã®
ã‚¹ãƒãƒ¼ãƒ„ï¼šé‡çƒã‚„ã‚µãƒƒã‚«ãƒ¼ã€ãƒãƒ¬ãƒ¼ãƒœãƒ¼ãƒ«ãªã©ã«é–¢ã‚ã‚‹ã‚‚ã®
ãã®ä»–ï¼šä¸Šè¨˜ã«å«ã¾ã‚Œãªã„ã‚‚ã®

å‡ºåŠ›å½¢å¼ã¯ã€ä¸‹ã®ã€Œç•ªå·. ã‚¿ã‚¤ãƒˆãƒ«ã€ä¸€è¦§ã«å¯¾ã—ã€å…¥åŠ›ã®ç•ªå·ã¨åŒã˜ç•ªå·ãƒ»åŒã˜ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãã®ã¾ã¾ç”¨ã„ã¦ã€
ã€Œç•ªå·\tã‚¿ã‚¤ãƒˆãƒ«\tãƒã‚¸ãƒ†ã‚£ãƒ–|ãƒã‚¬ãƒ†ã‚£ãƒ–|ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«\tã‚«ãƒ†ã‚´ãƒªã€
ã®4åˆ—TSVã§ã€å…¥åŠ›ã¨åŒã˜ä»¶æ•°ãƒ»åŒã˜é †åºã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
â€»ã‚¿ã‚¤ãƒˆãƒ«ã¯ä¸€åˆ‡å¤‰æ›´ãƒ»ä¿®æ­£ã—ãªã„ã“ã¨ã€‚ã‚«ãƒ†ã‚´ãƒªã¯ä¸¦è¨˜ã›ãšæœ€ã‚‚é–¢é€£æ€§ãŒé«˜ã„1ã¤ã ã‘ã‚’é¸ã¶ã“ã¨ã€‚
"""

def classify_with_gemini(titles):
    if not GEMINI_API_KEY:
        raise RuntimeError("GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€AIåˆ†é¡ã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚")
    import google.generativeai as genai
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(GEMINI_MODEL)

    results_by_idx = {}
    BATCH = 20
    idx_offset = 0
    for i in range(0, len(titles), BATCH):
        chunk = titles[i:i+BATCH]
        # ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆ1å§‹ã¾ã‚Šã€ãƒãƒƒãƒå†…ã¯ã‚ªãƒ•ã‚»ãƒƒãƒˆåŠ ç®—ï¼‰
        numbered = [f"{idx_offset+j+1}. {t}" for j, t in enumerate(chunk)]
        prompt = GEMINI_PROMPT + "\nç•ªå·ä»˜ãã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§:\n" + "\n".join(numbered)

        resp_text = ""
        for attempt in range(4):
            try:
                resp = model.generate_content(prompt)
                resp_text = (resp.text or "").strip()
                if resp_text:
                    break
            except Exception as e:
                wait = 2 + attempt * 3 + random.random() * 2
                print(f"âš ï¸ Gemini API ãƒªãƒˆãƒ©ã‚¤ {attempt+1}/4: {e}ï¼ˆå¾…æ©Ÿ {wait:.1f}sï¼‰")
                time.sleep(wait)
        if not resp_text:
            raise RuntimeError("Gemini ã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚")

        lines = [ln for ln in resp_text.splitlines() if ln.strip()]
        for ln in lines:
            parts = [p.strip() for p in ln.split("\t")]
            if len(parts) >= 4:
                # æœŸå¾…: ç•ªå·, ã‚¿ã‚¤ãƒˆãƒ«, åˆ¤å®š, ã‚«ãƒ†ã‚´ãƒª
                num_str, title_out, senti, cate = parts[0], parts[1], parts[2], parts[3]
                m = re.match(r"^\d+$", num_str)
                if not m:
                    # "12. " ãªã©ã«æ¥ã¦ã‚‚æ‹¾ãˆã‚‹ã‚ˆã†ã«
                    m2 = re.match(r"^(\d+)\.?", num_str)
                    if m2:
                        num_str = m2.group(1)
                    else:
                        continue
                idx = int(num_str)
                results_by_idx[idx] = (senti, cate)
        idx_offset += len(chunk)

    # ä¸¦ã³ã‚’å…ƒã®é †ã«å¾©å…ƒï¼ˆæ¬ æã¯ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«/ãã®ä»–ã§åŸ‹ã‚ã‚‹ï¼‰
    results = []
    for i in range(1, len(titles)+1):
        results.append(results_by_idx.get(i, ("ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "ãã®ä»–")))
    return results

# ========= Sheets =========
def open_sheet(spreadsheet_id: str):
    if not spreadsheet_id:
        raise ValueError("SPREADSHEET_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    blob = os.getenv("GCP_SERVICE_ACCOUNT_KEY", "")
    if not blob:
        raise ValueError("GCP_SERVICE_ACCOUNT_KEY ãŒç©ºã§ã™ï¼ˆã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONæœ¬æ–‡ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼‰ã€‚")
    try:
        data = json.loads(blob)
        creds = Credentials.from_service_account_info(data, scopes=["https://www.googleapis.com/auth/spreadsheets"])
    except Exception:
        # æ–‡å­—åˆ—ãŒJSONã§ãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æ›¸ãå‡ºã—ã¦èª­ã‚€
        path = "credentials.json"
        with open(path, "w", encoding="utf-8") as f:
            f.write(blob)
        creds = Credentials.from_service_account_file(path, scopes=["https://www.googleapis.com/auth/spreadsheets"])
    gc = gspread.authorize(creds)
    return gc.open_by_key(spreadsheet_id)

def upsert_single_sheet(sh, sheet_name: str, rows: list):
    headers = ["ã‚½ãƒ¼ã‚¹", "URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª"]
    try:
        ws = sh.worksheet(sheet_name)
        ws.clear()
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=sheet_name, rows=str(max(1000, len(rows)+10)), cols=str(len(headers)))
    ws.update("A1:G1", [headers])
    if rows:
        ws.update(f"A2:G{len(rows)+1}", rows)

# ========= ãƒ¡ã‚¤ãƒ³ =========
def main():
    now = datetime.now(JST)
    start, end, sheet_name = compute_window_and_sheet_name(now)
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {KEYWORD}")
    print(f"ğŸ“… æœŸé–“: {fmt_jst(start)}ã€œ{fmt_jst(end)} / ã‚·ãƒ¼ãƒˆ: {sheet_name}")

    # å–å¾—ï¼ˆé †åº: MSN â†’ Google â†’ Yahooï¼‰
    msn = fetch_msn(KEYWORD)
    print(f"MSN raw: {len(msn)}")

    google = fetch_google(KEYWORD)
    print(f"Google raw: {len(google)}")

    yahoo = fetch_yahoo(KEYWORD)
    print(f"Yahoo raw: {len(yahoo)}")

    # çµåˆï¼ˆé †åºç¶­æŒï¼‰ï¼† URLé‡è¤‡å…ˆå‹ã¡ï¼ˆMSNå„ªå…ˆï¼‰ï¼† ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ã¿
    merged = []
    seen = set()
    for row in (msn + google + yahoo):
        src, url, title, pub, origin = row
        if url in seen:
            continue
        seen.add(url)
        if not pub or not in_window(pub, start, end):
            continue
        merged.append(row)

    print(f"ğŸ“¦ ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(merged)} ä»¶")

    if not merged:
        # ä½•ã‚‚ç„¡ã„å ´åˆã§ã‚‚ã‚·ãƒ¼ãƒˆã¯ä½œæˆï¼ˆãƒ˜ãƒƒãƒ€ã®ã¿ï¼‰
        sh = open_sheet(SPREADSHEET_ID)
        upsert_single_sheet(sh, sheet_name, [])
        print("âš ï¸ æœŸé–“å†…ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # åˆ†é¡ï¼ˆç•ªå·+ã‚¿ã‚¤ãƒˆãƒ«ã§AIã«ä¾é ¼ï¼‰
    titles = [t for (_, _, t, _, _) in merged]
    labels = classify_with_gemini(titles)  # [(sentiment, category)]

    # å‡ºåŠ›æ•´å½¢
    def norm_sent(s):
        s = s.strip()
        if s.startswith("ãƒã‚¸"): return "ãƒã‚¸ãƒ†ã‚£ãƒ–"
        if s.startswith("ãƒã‚¬"): return "ãƒã‚¬ãƒ†ã‚£ãƒ–"
        if "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«" in s or "neutral" in s.lower(): return "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«"
        return s or "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«"

    rows = []
    for (src, url, title, pub, origin), (senti, cate) in zip(merged, labels):
        rows.append([src, url, title, pub, origin or "", norm_sent(senti), cate])

    sh = open_sheet(SPREADSHEET_ID)
    upsert_single_sheet(sh, sheet_name, rows)
    print(f"âœ… æ›¸ãè¾¼ã¿å®Œäº†: {sheet_name}ï¼ˆ{len(rows)}ä»¶ï¼‰")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("âŒ ã‚¨ãƒ©ãƒ¼:", e)
        traceback.print_exc()
        raise
