# -*- coding: utf-8 -*-
import os
import re
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime

import gspread
from google.oauth2.service_account import Credentials
import google.generativeai as genai

# ========= ç’°å¢ƒå¤‰æ•° =========
NEWS_KEYWORD = os.environ.get("NEWS_KEYWORD", "æ—¥ç”£")
SPREADSHEET_ID = os.environ.get("SPREADSHEET_ID")  # å¿…é ˆ
GCP_SERVICE_ACCOUNT_KEY = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")  # å¿…é ˆ(JSON)
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")  # ä»»æ„(æœªè¨­å®šãªã‚‰åˆ†é¡ã‚¹ã‚­ãƒƒãƒ—)

JST = timezone(timedelta(hours=9))

def now_jst() -> datetime:
    return datetime.now(JST)

def fmt_jst(dt: datetime) -> str:
    return dt.astimezone(JST).strftime("%Y/%m/%d %H:%M")

# ========= Google Sheets èªè¨¼ =========
def get_gspread_client():
    if not GCP_SERVICE_ACCOUNT_KEY:
        raise RuntimeError("GCP_SERVICE_ACCOUNT_KEY ãŒæœªè¨­å®šã§ã™ã€‚")
    creds_json = json.loads(GCP_SERVICE_ACCOUNT_KEY)
    scopes = ["https://www.googleapis.com/auth/spreadsheets"]
    credentials = Credentials.from_service_account_info(creds_json, scopes=scopes)
    return gspread.authorize(credentials)

# ========= Googleãƒ‹ãƒ¥ãƒ¼ã‚¹(RSS) =========
def fetch_google_news(keyword: str):
    url = f"https://news.google.com/rss/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    r = requests.get(url, timeout=15)
    r.raise_for_status()
    # ã¾ãš lxml-xmlã€ãƒ€ãƒ¡ãªã‚‰ xmlã€æœ€å¾Œã« html.parser ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    soup = None
    for parser in ("lxml-xml", "xml", "html.parser"):
        try:
            soup = BeautifulSoup(r.text, parser)
            break
        except Exception:
            soup = None
    if soup is None:
        return []

    items = []
    for it in soup.find_all("item"):
        title = (it.title.text if it.title else "").strip()
        link = (it.link.text if it.link else "").strip()
        pub_raw = it.pubDate.text.strip() if it.pubDate else ""
        source = (it.source.text if it.source else "Google").strip()
        pub = ""
        if pub_raw:
            try:
                dt = parsedate_to_datetime(pub_raw)  # aware
                pub = fmt_jst(dt)
            except Exception:
                pub = ""
        if title and link:
            items.append(("Google", link, title, pub, source))
    return items

# ========= MSNãƒ‹ãƒ¥ãƒ¼ã‚¹(ç°¡æ˜“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—) =========
def fetch_msn_news(keyword: str):
    url = f"https://www.bing.com/news/search?q={keyword}&cc=jp"
    r = requests.get(url, timeout=15)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")
    items = []
    for a in soup.select("a.title, h2 a, h3 a"):
        link = a.get("href") or ""
        title = a.get_text(strip=True)
        if not title or not link:
            continue
        # MSNã¯å®‰å®šã—ãŸæ™‚åˆ»æŠ½å‡ºãŒé›£ã—ã„ãŸã‚å–å¾—æ™‚åˆ»ã‚’æ¡ç”¨
        pub = fmt_jst(now_jst())
        source = "MSN"
        items.append(("MSN", link, title, pub, source))
    return items

# ========= Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ + ã‚³ãƒ¡ãƒ³ãƒˆæ•° =========
YAHOO_COMMENT_RE = re.compile(r"ã‚³ãƒ¡ãƒ³ãƒˆ[ï¼ˆ(]\s*([0-9,]+)\s*[)ï¼‰]")

def fetch_yahoo_comment_count(url: str) -> int:
    if "news.yahoo.co.jp" not in url:
        return 0
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        text = BeautifulSoup(r.text, "html.parser").get_text(" ", strip=True)
        m = YAHOO_COMMENT_RE.search(text)
        if m:
            return int(m.group(1).replace(",", ""))
    except Exception:
        pass
    return 0

def fetch_yahoo_news(keyword: str):
    url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    r = requests.get(url, timeout=15)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")
    items = []
    for a in soup.select("a.newsFeed_item_link"):
        link = a.get("href") or ""
        title = (a.get("title") or a.get_text(strip=True) or "").strip()
        if not title or not link:
            continue
        pub = fmt_jst(now_jst())  # æ¤œç´¢çµæœé¢ã‹ã‚‰ã¯æ™‚åˆ»å–å¾—å›°é›£ãªãŸã‚å–å¾—æ™‚åˆ»ã‚’ä½¿ç”¨
        source = "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹"
        cmt = fetch_yahoo_comment_count(link)
        items.append(("Yahoo", link, title, pub, source, cmt))
    return items

# ========= Gemini(ã‚¿ã‚¤ãƒˆãƒ«â†’ãƒã‚¸/ãƒã‚¬ï¼†ã‚«ãƒ†ã‚´ãƒª) =========
def analyze_titles_gemini(titles: list[str]) -> dict[str, tuple[str, str]]:
    if not GEMINI_API_KEY or not titles:
        return {}
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel("gemini-1.5-flash")

    prompt = """ã‚ãªãŸã¯æ•è…•é›‘èªŒè¨˜è€…ã§ã™ã€‚ ä»¥ä¸‹ã®Webãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã«ã¤ã„ã¦ã€
â‘ ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã‚’åˆ¤åˆ¥
â‘¡è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’åˆ¤åˆ¥ï¼ˆä¼šç¤¾ã€è»Šã€è»Šï¼ˆç«¶åˆï¼‰ã€æŠ€è¡“ï¼ˆEVï¼‰ã€æŠ€è¡“ï¼ˆe-POWERï¼‰ã€æŠ€è¡“ï¼ˆe-4ORCEï¼‰ã€æŠ€è¡“ï¼ˆAD/ADASï¼‰ã€æŠ€è¡“ã€ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚¹ãƒãƒ¼ãƒ„ã€æ ªå¼ã€æ”¿æ²»ãƒ»çµŒæ¸ˆã€ã‚¹ãƒãƒ¼ãƒ„ã€ãã®ä»–ï¼‰
å‡ºåŠ›ã¯ JSON é…åˆ—ã§ã€å„è¦ç´ ãŒ {"title": "...", "sentiment": "ãƒã‚¸ãƒ†ã‚£ãƒ–|ãƒã‚¬ãƒ†ã‚£ãƒ–|ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "category": "..."} ã®å½¢å¼ã ã‘ã§è¿”ã—ã¦ãã ã•ã„ã€‚
ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§:
""" + "\n".join([f"- {t}" for t in titles])

    try:
        resp = model.generate_content(prompt)
        text = (resp.text or "").strip()
        data = json.loads(text)
        out = {}
        for d in data:
            if isinstance(d, dict) and "title" in d:
                out[d["title"]] = (d.get("sentiment", ""), d.get("category", ""))
        return out
    except Exception as e:
        print(f"Geminiè§£æå¤±æ•—: {e}")
        return {}

# ========= é›†ç´„ï¼ˆæ˜¨æ—¥15:00ã€œä»Šæ—¥14:59ã€ã‚·ãƒ¼ãƒˆå=ä»Šæ—¥ã®YYMMDDï¼‰ =========
def build_daily_sheet(sh, all_items: list[tuple]):
    now = now_jst()
    today_1500 = now.replace(hour=15, minute=0, second=0, microsecond=0)
    start = today_1500 - timedelta(days=1)  # æ˜¨æ—¥15:00
    end = today_1500                      # ä»Šæ—¥14:59:59 ã¾ã§ï¼ˆ< end åˆ¤å®šï¼‰
    sheet_name = now.strftime("%y%m%d")

    filtered = {"MSN": [], "Google": [], "Yahoo": []}
    no_date = 0
    for item in all_items:
        source = item[0]
        pub = item[3]  # "YYYY/MM/DD HH:MM" æœŸå¾…
        try:
            dt = datetime.strptime(pub, "%Y/%m/%d %H:%M").replace(tzinfo=JST)
            if start <= dt < end and source in filtered:
                filtered[source].append(item)
        except Exception:
            no_date += 1

    # ã‚·ãƒ¼ãƒˆç”¨æ„
    try:
        ws = sh.worksheet(sheet_name)
        ws.clear()
    except Exception:
        ws = sh.add_worksheet(title=sheet_name, rows="2000", cols="10")

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    ws.update("A1:H1", [["ã‚½ãƒ¼ã‚¹", "URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª"]])

    # ä¸¦ã³é †ï¼šMSNâ†’Googleâ†’Yahoo
    ordered = filtered["MSN"] + filtered["Google"] + filtered["Yahoo"]

    # ã‚¿ã‚¤ãƒˆãƒ« â†’ Gemini çµæœ
    titles = [row[2] for row in ordered]
    gemini_map = analyze_titles_gemini(titles)

    rows = []
    for row in ordered:
        # rowã®å½¢ï¼šMSN/Google: (src, url, title, pub, origin)
        # Yahoo: (src, url, title, pub, origin, comment)
        source, url, title, pub, origin = row[:5]
        comment = row[5] if len(row) > 5 else ""
        senti, cate = gemini_map.get(title, ("", ""))
        rows.append([source, url, title, pub, origin, comment, senti, cate])

    if rows:
        ws.update(f"A2:H{len(rows)+1}", rows)

    print(f"ğŸ•’ é›†ç´„æœŸé–“: {start.strftime('%Y/%m/%d %H:%M')} ã€œ {(end - timedelta(minutes=1)).strftime('%Y/%m/%d %H:%M')} â†’ ã‚·ãƒ¼ãƒˆå: {sheet_name}")
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿çµæœ: MSN={len(filtered['MSN'])}, Google={len(filtered['Google'])}, Yahoo={len(filtered['Yahoo'])}, æ—¥ä»˜ç„¡ã—ã‚¹ã‚­ãƒƒãƒ—={no_date}")
    print(f"âœ… é›†ç´„ã‚·ãƒ¼ãƒˆ {sheet_name}: {len(rows)} ä»¶")
    return sheet_name

# ========= Main =========
def main():
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {NEWS_KEYWORD}")
    print(f"ğŸ“„ SPREADSHEET_ID: {SPREADSHEET_ID}")
    if not SPREADSHEET_ID:
        raise RuntimeError("SPREADSHEET_ID ãŒæœªè¨­å®šã§ã™ã€‚")

    gc = get_gspread_client()
    sh = gc.open_by_key(SPREADSHEET_ID)
    print(f"ğŸ“˜ Opened spreadsheet title: {sh.title}")

    print("\n--- å–å¾— ---")
    google_items = fetch_google_news(NEWS_KEYWORD)
    yahoo_items = fetch_yahoo_news(NEWS_KEYWORD)
    msn_items = fetch_msn_news(NEWS_KEYWORD)

    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(google_items)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾— {sum(1 for i in google_items if i[3])} ä»¶ï¼‰")
    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(yahoo_items)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾— {sum(1 for i in yahoo_items if i[3])} ä»¶ï¼‰")
    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(msn_items)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾—/æ¨å®š {sum(1 for i in msn_items if i[3])} ä»¶ï¼‰")

    all_items = msn_items + google_items + yahoo_items  # å¾Œã§å†ã‚½ãƒ¼ãƒˆã™ã‚‹ãŒãƒ­ã‚°ç”¨

    print("\n--- é›†ç´„ï¼ˆã¾ã¨ã‚ã‚·ãƒ¼ãƒˆã®ã¿ / Aåˆ—=ã‚½ãƒ¼ã‚¹ / é †=MSNâ†’Googleâ†’Yahooï¼‰ ---")
    build_daily_sheet(sh, all_items)

if __name__ == "__main__":
    main()
