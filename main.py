# -*- coding: utf-8 -*-
import os
import re
import json
import time
import random
import requests
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime

from bs4 import BeautifulSoup
import gspread
import gspread.exceptions

# Selenium / Chrome
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# === Gemini ===
import google.generativeai as genai

# =======================
# è¨­å®š
# =======================
KEYWORD = os.environ.get("NEWS_KEYWORD", "æ—¥ç”£")  # å¿…è¦ã«å¿œã˜ã¦ Actions ã® env ã§ä¸Šæ›¸ãå¯
SPREADSHEET_ID = "1Vs4Cx8QPN4H2NOgtwaviOCe8zBTpUNDgJjqkHr51IZE"
JST = timezone(timedelta(hours=9))

# å‡ºåŠ›åˆ—ï¼ˆAã€œGï¼‰
OUTPUT_HEADERS = ["ã‚½ãƒ¼ã‚¹", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª"]

# Gemini ãƒ¢ãƒ‡ãƒ«åï¼ˆé€Ÿã•é‡è¦–: 1.5-flash / ç²¾åº¦é‡è¦–: 1.5-proï¼‰
GEMINI_MODEL_NAME = os.environ.get("GEMINI_MODEL_NAME", "gemini-1.5-flash")

# =======================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =======================
def format_datetime(dt_obj: datetime) -> str:
    return dt_obj.astimezone(JST).strftime("%Y/%m/%d %H:%M")

def try_parse_jst_datetime(s: str):
    """ "YYYY/MM/DD HH:MM" ãªã©ã‚’JST datetimeã«ã€‚å¤±æ•—ãªã‚‰ None """
    s = (s or "").strip()
    for fmt in ["%Y/%m/%d %H:%M", "%Y/%m/%d", "%Y-%m-%d %H:%M", "%Y-%m-%d"]:
        try:
            dt = datetime.strptime(s, fmt)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=JST)
            return dt.astimezone(JST)
        except Exception:
            continue
    return None

def parse_relative_time(pub_label: str, base_time: datetime) -> str:
    """ MSNãªã©ç›¸å¯¾è¡¨è¨˜ã‚’çµ¶å¯¾(JST)ã¸ã€‚"""
    label = (pub_label or "").strip()
    try:
        m = re.search(r"(\d+)", label)
        n = int(m.group(1)) if m else None

        # æ—¥æœ¬èª/è‹±èªã©ã¡ã‚‰ã‚‚ã‚†ã‚‹ãå¯¾å¿œ
        if ("åˆ†å‰" in label or "minute" in label) and n is not None:
            return format_datetime(base_time - timedelta(minutes=n))
        if ("æ™‚é–“å‰" in label or "hour" in label) and n is not None:
            return format_datetime(base_time - timedelta(hours=n))
        if ("æ—¥å‰" in label or "day" in label) and n is not None:
            return format_datetime(base_time - timedelta(days=n))

        # "8/20" ã®ã‚ˆã†ãªè¡¨è¨˜
        m2 = re.match(r"(\d{1,2})/(\d{1,2})", label)
        if m2:
            month, day = int(m2.group(1)), int(m2.group(2))
            dt = datetime(year=base_time.year, month=month, day=day, tzinfo=JST)
            return format_datetime(dt)
    except Exception:
        pass
    return "å–å¾—ä¸å¯"

def get_last_modified_datetime(url: str) -> str:
    try:
        res = requests.head(url, timeout=5)
        if "Last-Modified" in res.headers:
            dt = parsedate_to_datetime(res.headers["Last-Modified"])
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return format_datetime(dt.astimezone(JST))
    except Exception:
        pass
    return "å–å¾—ä¸å¯"

def setup_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    # å®‰å®šç”¨ï¼šç”»åƒèª­ã¿è¾¼ã¿ã‚ªãƒ•ãªã©ã‚’å…¥ã‚ŒãŸã„å ´åˆã¯ã“ã“ã«è¿½åŠ 
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    return driver

# =======================
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘
# =======================
def get_google_news(keyword: str) -> list[dict]:
    driver = setup_driver()
    url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    driver.get(url)
    time.sleep(5)
    for _ in range(3):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.2)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data = []
    for art in soup.find_all("article"):
        try:
            a_tag = art.select_one("a.JtKRv")
            time_tag = art.select_one("time.hvbAAd")
            source_tag = art.select_one("div.vr1PYe")

            if not a_tag or not time_tag:
                continue

            title = a_tag.get_text(strip=True)
            href = a_tag.get("href", "")
            url = "https://news.google.com" + href[1:] if href.startswith("./") else href

            # Googleã¯UTCã®ISOè¡¨è¨˜
            iso = time_tag.get("datetime")
            dt = datetime.strptime(iso, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc).astimezone(JST)
            pub = format_datetime(dt)

            source_name = source_tag.get_text(strip=True) if source_tag else "Google"
            data.append({"ã‚½ãƒ¼ã‚¹": "MSN" if False else "Google",  # ä¿é™º: å¤‰ãªç½®æ›å›é¿
                         "ã‚¿ã‚¤ãƒˆãƒ«": title,
                         "URL": url,
                         "æŠ•ç¨¿æ—¥": pub,
                         "å¼•ç”¨å…ƒ": source_name})
        except Exception:
            continue
    # ã‚½ãƒ¼ã‚¹åä¿®æ­£ï¼ˆä¸Šã®å¤‰ãªä¿é™ºã‚’ç„¡åŠ¹åŒ–ï¼‰
    for d in data:
        d["ã‚½ãƒ¼ã‚¹"] = "Google"
    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(data)} ä»¶")
    return data

def get_yahoo_news(keyword: str) -> list[dict]:
    """ Yahooå´ã®DOMå¤‰åŒ–ã«å¼·ã„å–ã‚Šæ–¹ï¼šè¨˜äº‹URLãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ‹¾ã† """
    driver = setup_driver()
    url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(url)
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data = []
    # Yahooè¨˜äº‹ãƒªãƒ³ã‚¯ã¯ "https://news.yahoo.co.jp/articles/xxxxx" ãŒåŸºæœ¬
    links = soup.select("a[href^='https://news.yahoo.co.jp/articles/']")
    seen_urls = set()
    for a in links:
        try:
            title = a.get_text(strip=True)
            url = a.get("href")
            if not title or not url or url in seen_urls:
                continue
            seen_urls.add(url)

            parent_li = a.find_parent("li")
            # æŠ•ç¨¿æ—¥
            date_str = "å–å¾—ä¸å¯"
            if parent_li:
                time_tag = parent_li.find("time")
                if time_tag:
                    date_str = time_tag.get_text(strip=True)
                    date_str = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)", "", date_str).strip()

            # å¼•ç”¨å…ƒï¼ˆåª’ä½“åï¼‰
            source_name = "Yahoo"
            if parent_li:
                # è¦‹å‡ºã—å‘¨è¾ºã®çŸ­æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æ‹¾ã†ï¼ˆåª’ä½“åå€™è£œï¼‰
                for s in parent_li.select("span, div"):
                    t = s.get_text(strip=True)
                    if t and 2 <= len(t) <= 20 and re.search(r"[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥A-Za-z]", t) and "è¨˜äº‹" not in t:
                        source_name = t
                        break

            data.append({"ã‚½ãƒ¼ã‚¹": "Yahoo",
                         "ã‚¿ã‚¤ãƒˆãƒ«": title,
                         "URL": url,
                         "æŠ•ç¨¿æ—¥": date_str or "å–å¾—ä¸å¯",
                         "å¼•ç”¨å…ƒ": source_name})
        except Exception:
            continue

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(data)} ä»¶")
    return data

def get_msn_news(keyword: str) -> list[dict]:
    now = datetime.now(JST)
    driver = setup_driver()
    # Bing Newsï¼ˆæ–°ã—ã„é †ï¼‰
    url = f"https://www.bing.com/news/search?q={keyword}&qft=sortbydate%3d'1'&form=YFNR"
    driver.get(url)
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data = []
    cards = soup.select("div.news-card")
    for card in cards:
        try:
            title = (card.get("data-title") or "").strip()
            url = (card.get("data-url") or "").strip()
            source_name = (card.get("data-author") or "").strip() or "MSN"

            pub_label = ""
            pub_tag = card.find("span", attrs={"aria-label": True})
            if pub_tag and pub_tag.has_attr("aria-label"):
                pub_label = pub_tag["aria-label"].strip()

            pub = parse_relative_time(pub_label, now)
            if pub == "å–å¾—ä¸å¯" and url:
                pub = get_last_modified_datetime(url)

            if title and url:
                data.append({"ã‚½ãƒ¼ã‚¹": "MSN",
                             "ã‚¿ã‚¤ãƒˆãƒ«": title,
                             "URL": url,
                             "æŠ•ç¨¿æ—¥": pub,
                             "å¼•ç”¨å…ƒ": source_name})
        except Exception:
            continue
    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(data)} ä»¶")
    return data

# =======================
# æ™‚é–“çª“ãƒ»ã‚·ãƒ¼ãƒˆå
# =======================
def compute_window(now_jst: datetime):
    """
    ã€Œå‰æ—¥15:00ã€œå½“æ—¥14:59ã€ã®é›†è¨ˆçª“ã¨ã‚·ãƒ¼ãƒˆå(YYMMDD)ã‚’è¿”ã™ã€‚
    """
    today = now_jst.date()
    end = datetime(today.year, today.month, today.day, 14, 59, 59, tzinfo=JST)
    start = end - timedelta(days=1) + timedelta(seconds=1)  # å‰æ—¥15:00:00
    sheet_name = end.strftime("%y%m%d")
    return start, end, sheet_name

def in_window(dt_str: str, start: datetime, end: datetime) -> bool:
    dt = try_parse_jst_datetime(dt_str)
    if dt is None:
        return False
    return start <= dt <= end

# =======================
# Google Sheets
# =======================
def service_account():
    """
    ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY (JSONæ–‡å­—åˆ—) ã‚’å„ªå…ˆã€‚
    ç„¡ã‘ã‚Œã° credentials.json ã‚’èª­ã‚€ã€‚
    """
    env_str = os.environ.get("GCP_SERVICE_ACCOUNT_KEY", "")
    if env_str:
        try:
            creds = json.loads(env_str)
            return gspread.service_account_from_dict(creds)
        except Exception as e:
            raise RuntimeError(f"ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    else:
        return gspread.service_account(filename="credentials.json")

# =======================
# Geminiï¼ˆã‚¿ã‚¤ãƒˆãƒ«åˆ†é¡ï¼‰
# =======================
GEMINI_PROMPT = """
ã‚ãªãŸã¯æ•è…•é›‘èªŒè¨˜è€…ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã”ã¨ã«ã€æ¬¡ã®äºŒã¤ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
1) ãƒã‚¸ãƒã‚¬åˆ¤å®š: ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã€ã‹ã‚‰ä¸€ã¤
2) ã‚«ãƒ†ã‚´ãƒªãƒ¼: æ¬¡ã‹ã‚‰å¿…ãšä¸€ã¤ã ã‘é¸ã‚“ã§ãã ã•ã„ï¼ˆä¸¦è¨˜ç¦æ­¢ï¼‰
   - ä¼šç¤¾ï¼ˆãƒ‹ãƒƒã‚µãƒ³ã€ãƒˆãƒ¨ã‚¿ã€ãƒ›ãƒ³ãƒ€ã€ã‚¹ãƒãƒ«ã€ãƒãƒ„ãƒ€ã€ã‚¹ã‚ºã‚­ã€ãƒŸãƒ„ãƒ“ã‚·ã€ãƒ€ã‚¤ãƒãƒ„ã®å ´åˆã¯ (ä¼æ¥­å) ã‚’ä»˜è¨˜ï¼‰
   - è»Šï¼ˆæ–°å‹/ç¾è¡Œ/æ—§å‹ + åç§°ã‚’ () ã§è¨˜è¼‰ã€‚æ—¥ç”£ä»¥å¤–ã®è»Šãªã‚‰ã€Œè»Šï¼ˆç«¶åˆï¼‰ã€ã¨ã—ã€()ã«åç§°ï¼‰
   - æŠ€è¡“ï¼ˆEVï¼‰
   - æŠ€è¡“ï¼ˆe-POWERï¼‰
   - æŠ€è¡“ï¼ˆe-4ORCEï¼‰
   - æŠ€è¡“ï¼ˆAD/ADASï¼‰
   - æŠ€è¡“ï¼ˆãã®ä»–ï¼‰
   - ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚¹ãƒãƒ¼ãƒ„
   - æ ªå¼
   - æ”¿æ²»ãƒ»çµŒæ¸ˆ
   - ã‚¹ãƒãƒ¼ãƒ„
   - ãã®ä»–

åˆ¶ç´„:
- å‡ºåŠ›ã¯å¿…ãš JSON é…åˆ—ã€‚å„è¦ç´ ã¯ {"title": <å…¥åŠ›ã‚¿ã‚¤ãƒˆãƒ«>, "sentiment": "ãƒã‚¸ãƒ†ã‚£ãƒ–|ãƒã‚¬ãƒ†ã‚£ãƒ–|ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "category": "<ä¸Šè¨˜ã®ã„ãšã‚Œã‹>"} ã®å½¢ã€‚
- å…¥åŠ›ã‚¿ã‚¤ãƒˆãƒ«ã¯ä¸€åˆ‡å¤‰æ›´ã›ãšã€ãã®ã¾ã¾ "title" ã«å…¥ã‚Œã¦ãã ã•ã„ã€‚
- å¿…ãšã‚¿ã‚¤ãƒˆãƒ«æ•°ã¨åŒã˜ä»¶æ•°ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
"""

def init_gemini():
    api_key = os.environ.get("GEMINI_API_KEY", "")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY ãŒæœªè¨­å®šã§ã™ã€‚")
    genai.configure(api_key=api_key)
    return genai.GenerativeModel(GEMINI_MODEL_NAME)

def classify_titles_gemini(titles: list[str]) -> dict:
    """
    titles ã®å„ã‚¿ã‚¤ãƒˆãƒ«ã«å¯¾ã— {"sentiment":..., "category":...} ã‚’è¿”ã™ dict ã‚’ä½œã‚‹ã€‚
    å¤±æ•—æ™‚ã¯ ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ« / ãã®ä»–ã€‚
    """
    model = init_gemini()
    default = {"sentiment": "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "category": "ãã®ä»–"}
    if not titles:
        return {}

    result_map = {}
    BATCH = 50
    for i in range(0, len(titles), BATCH):
        chunk = titles[i:i+BATCH]
        payload = {"titles": chunk}
        prompt = GEMINI_PROMPT + "\nå…¥åŠ›ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§(JSON)ï¼š\n" + json.dumps(payload, ensure_ascii=False)
        try:
            resp = model.generate_content(prompt)
            text = (resp.text or "").strip()
            # JSONæ¤œå‡ºï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–ï¼‰
            m = re.search(r"\[.*\]", text, flags=re.DOTALL)
            json_str = m.group(0) if m else text
            data = json.loads(json_str)
            if isinstance(data, list):
                for item in data:
                    t = item.get("title", "")
                    sent = (item.get("sentiment", "") or "").strip() or default["sentiment"]
                    cat = (item.get("category", "") or "").strip() or default["category"]
                    result_map[t] = {"sentiment": sent, "category": cat}
            else:
                for t in chunk:
                    result_map[t] = default
        except Exception:
            for t in chunk:
                result_map[t] = default
        time.sleep(0.5)  # rate å¯¾ç­–

    return result_map

# =======================
# æ›¸ãè¾¼ã¿
# =======================
def write_unified_sheet(articles: list[dict], spreadsheet_id: str, sheet_name: str):
    gc = service_account()

    # 5å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤ï¼ˆ429å¯¾ç­–ï¼‰
    for attempt in range(5):
        try:
            sh = gc.open_by_key(spreadsheet_id)
            try:
                ws = sh.worksheet(sheet_name)
            except gspread.exceptions.WorksheetNotFound:
                ws = sh.add_worksheet(title=sheet_name, rows="200", cols=str(len(OUTPUT_HEADERS)))
                ws.append_row(OUTPUT_HEADERS, value_input_option="USER_ENTERED")

            # æ—¢å­˜URLã®é‡è¤‡å›é¿
            existing = ws.get_all_values()
            existing_urls = set()
            if existing and len(existing) > 1:
                for row in existing[1:]:
                    if len(row) >= 3 and row[2]:
                        existing_urls.add(row[2])

            # === ã‚¿ã‚¤ãƒˆãƒ«åˆ†é¡ï¼ˆGeminiï¼‰ ===
            titles = [a["ã‚¿ã‚¤ãƒˆãƒ«"] for a in articles if a.get("ã‚¿ã‚¤ãƒˆãƒ«")]
            title_to_cls = classify_titles_gemini(titles)

            new_rows = []
            for a in articles:
                url = a.get("URL", "")
                if not url or url in existing_urls:
                    continue
                title = a.get("ã‚¿ã‚¤ãƒˆãƒ«", "")
                cls = title_to_cls.get(title, {"sentiment": "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "category": "ãã®ä»–"})
                new_rows.append([
                    a.get("ã‚½ãƒ¼ã‚¹", ""),           # A: ã‚½ãƒ¼ã‚¹ (MSN/Google/Yahoo)
                    title,                         # B: ã‚¿ã‚¤ãƒˆãƒ«
                    url,                           # C: URL
                    a.get("æŠ•ç¨¿æ—¥", ""),            # D: æŠ•ç¨¿æ—¥ (JST)
                    a.get("å¼•ç”¨å…ƒ", ""),            # E: å¼•ç”¨å…ƒï¼ˆåª’ä½“åï¼‰
                    cls["sentiment"],              # F: ãƒã‚¸ãƒã‚¬
                    cls["category"],               # G: ã‚«ãƒ†ã‚´ãƒª
                ])

            if new_rows:
                ws.append_rows(new_rows, value_input_option="USER_ENTERED")
                print(f"âœ… {len(new_rows)} ä»¶ã‚’ '{sheet_name}' ã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
            else:
                print("âš ï¸ è¿½è¨˜å¯¾è±¡ãªã—ï¼ˆé‡è¤‡ or è©²å½“æœŸé–“å¤–ï¼‰")

            return
        except gspread.exceptions.APIError as e:
            print(f"âš ï¸ Google API Error (attempt {attempt+1}/5): {e}")
            time.sleep(5 + random.random() * 5)

    raise RuntimeError("âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿ã«å¤±æ•—ï¼ˆ5å›è©¦è¡Œï¼‰")

# =======================
# ãƒ¡ã‚¤ãƒ³
# =======================
def main():
    now_jst = datetime.now(JST)
    start, end, sheet_name = compute_window(now_jst)
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {KEYWORD}")
    print(f"ğŸ“… åé›†ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {start.strftime('%Y/%m/%d %H:%M:%S')} ã€œ {end.strftime('%Y/%m/%d %H:%M:%S')} (JST)")
    print(f"ğŸ—‚ å‡ºåŠ›ã‚·ãƒ¼ãƒˆå: {sheet_name}")

    # å–å¾—ï¼ˆMSNâ†’Googleâ†’Yahoo ã®é †ã§å¾Œæ®µã®å‡ºåŠ›é †ã‚‚æ‹…ä¿ï¼‰
    m_list = get_msn_news(KEYWORD)
    g_list = get_google_news(KEYWORD)
    y_list = get_yahoo_news(KEYWORD)

    # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ + URLé‡è¤‡æ’é™¤ï¼ˆé †ç•ªã¯ MSN â†’ Google â†’ Yahooï¼‰
    all_articles = []
    seen = set()
    for src_list in [m_list, g_list, y_list]:  # å‡ºåŠ›é †å›ºå®š
        for a in src_list:
            url = a.get("URL")
            if not url or url in seen:
                continue
            if a.get("æŠ•ç¨¿æ—¥") and in_window(a["æŠ•ç¨¿æ—¥"], start, end):
                all_articles.append(a)
                seen.add(url)

    print(f"ğŸ§® æœŸé–“è©²å½“ä»¶æ•°: {len(all_articles)}")

    if all_articles:
        write_unified_sheet(all_articles, SPREADSHEET_ID, sheet_name)
    else:
        print("âš ï¸ è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

if __name__ == "__main__":
    main()
