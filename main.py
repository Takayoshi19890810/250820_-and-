import os
import re
import json
import unicodedata
from datetime import datetime, timedelta, time as dtime
import requests
from bs4 import BeautifulSoup
import gspread
from google.oauth2.service_account import Credentials
import google.generativeai as genai

# ========= ç’°å¢ƒå¤‰æ•° =========
NEWS_KEYWORD = os.getenv("NEWS_KEYWORD", "æ—¥ç”£")
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
GCP_KEY = os.getenv("GCP_SERVICE_ACCOUNT_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ========= Google Sheets èªè¨¼ =========
def get_spreadsheet(spreadsheet_id: str):
    creds = None
    if GCP_KEY:
        key_data = json.loads(GCP_KEY)
        creds = Credentials.from_service_account_info(
            key_data,
            scopes=["https://www.googleapis.com/auth/spreadsheets"]
        )
    else:
        raise RuntimeError("GCP_SERVICE_ACCOUNT_KEY ãŒæœªè¨­å®šã§ã™")

    client = gspread.authorize(creds)
    return client.open_by_key(spreadsheet_id)

# ========= Google News =========
def fetch_google_news(keyword):
    url = f"https://news.google.com/rss/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    res = requests.get(url, timeout=15)
    res.raise_for_status()
    try:
        soup = BeautifulSoup(res.text, "lxml-xml")
    except Exception:
        soup = BeautifulSoup(res.text, "xml")

    items = []
    for item in soup.find_all("item"):
        title = (item.title.text or "").strip()
        link = (item.link.text or "").strip()
        pubdate_raw = item.pubDate.text.strip() if item.pubDate else ""
        source = item.source.text.strip() if item.source else "Googleãƒ‹ãƒ¥ãƒ¼ã‚¹"
        items.append(("Google", link, title, pubdate_raw, source))
    return items

# ========= MSN News =========
def fetch_msn_news(keyword):
    url = f"https://www.bing.com/news/search?q={keyword}&format=RSS&cc=JP"
    res = requests.get(url, timeout=15)
    res.raise_for_status()
    try:
        soup = BeautifulSoup(res.text, "lxml-xml")
    except Exception:
        soup = BeautifulSoup(res.text, "xml")

    items = []
    for item in soup.find_all("item"):
        title = (item.title.text or "").strip()
        link = (item.link.text or "").strip()
        pubdate_raw = item.pubDate.text.strip() if item.pubDate else ""
        source = item.source.text.strip() if item.source else "MSNãƒ‹ãƒ¥ãƒ¼ã‚¹"
        items.append(("MSN", link, title, pubdate_raw, source))
    return items

# ========= Yahoo News =========
def fetch_yahoo_news(keyword):
    url = f"https://news.yahoo.co.jp/rss/search?p={keyword}"
    res = requests.get(url, timeout=15)
    res.raise_for_status()
    try:
        soup = BeautifulSoup(res.text, "lxml-xml")
    except Exception:
        soup = BeautifulSoup(res.text, "xml")

    items = []
    for item in soup.find_all("item"):
        title = (item.title.text or "").strip()
        link = (item.link.text or "").strip()
        pubdate_raw = item.pubDate.text.strip() if item.pubDate else ""
        source = "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹"
        items.append(("Yahoo", link, title, pubdate_raw, source))
    return items

# ========= æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ =========
def compute_window(now_jst: datetime):
    today = now_jst.date()
    today_1500 = datetime.combine(today, dtime(hour=15, minute=0))
    start = today_1500 - timedelta(days=1)
    end   = today_1500 - timedelta(seconds=1)
    label = today.strftime("%y%m%d")
    return start, end, label

def parse_pubdate(text: str):
    if not text:
        return None
    for fmt in [
        "%a, %d %b %Y %H:%M:%S %Z",
        "%a, %d %b %Y %H:%M:%S %z",
        "%Y/%m/%d %H:%M",
        "%Y-%m-%d %H:%M:%S",
    ]:
        try:
            return datetime.strptime(text, fmt)
        except Exception:
            continue
    return None

# ========= Gemini API =========
def analyze_titles_gemini(titles):
    if not GEMINI_API_KEY or not titles:
        return {}
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel("gemini-1.5-flash")

    prompt = """ã‚ãªãŸã¯æ•è…•é›‘èªŒè¨˜è€…ã§ã™ã€‚ ä»¥ä¸‹ã®Webãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã«ã¤ã„ã¦ã€
â‘ ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã‚’åˆ¤åˆ¥
â‘¡è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’åˆ¤åˆ¥ã€‚ãƒ«ãƒ¼ãƒ«:
ä¼šç¤¾ï¼šä¼æ¥­ã®æ–½ç­–ã‚„ç”Ÿç”£ã€è²©å£²å°æ•°ãªã©ã€‚ãƒ‹ãƒƒã‚µãƒ³/ãƒˆãƒ¨ã‚¿/ãƒ›ãƒ³ãƒ€/ã‚¹ãƒãƒ«/ãƒãƒ„ãƒ€/ã‚¹ã‚ºã‚­/ãƒŸãƒ„ãƒ“ã‚·/ãƒ€ã‚¤ãƒãƒ„ãªã‚‰ (ä¼šç¤¾å) ã‚’ä»˜è¨˜ã€‚ä»–ã¯ ãã®ä»–ã€‚
è»Šï¼šã‚¯ãƒ«ãƒåç§°ãŒå«ã¾ã‚Œã‚‹å ´åˆã€‚æ–°å‹/ç¾è¡Œ/æ—§å‹+åç§°ã‚’()ä»˜ã§ã€‚æ—¥ç”£ä»¥å¤–ã¯ è»Šï¼ˆç«¶åˆï¼‰ã€‚
æŠ€è¡“ï¼ˆEV/e-POWER/e-4ORCE/AD/ADAS/ãã®ä»–ï¼‰
ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚¹ãƒãƒ¼ãƒ„ã€æ ªå¼ã€æ”¿æ²»ãƒ»çµŒæ¸ˆã€ã‚¹ãƒãƒ¼ãƒ„ã€ãã®ä»–ã€‚
å‡ºåŠ›ã¯ JSON é…åˆ— [{"title":"...","sentiment":"...","category":"..."}] ã®å½¢å¼ã§ã€‚
ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§:
""" + "\n".join([f"- {t}" for t in titles])

    try:
        resp = model.generate_content(prompt)
        text = resp.text.strip()
        data = json.loads(text)
        return {d["title"]: (d["sentiment"], d["category"]) for d in data}
    except Exception as e:
        print(f"Geminiè§£æå¤±æ•—: {e}")
        return {}

# ========= ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¤å®šå¼·åŒ–ãƒ­ã‚¸ãƒƒã‚¯ =========
def _norm(s: str) -> str:
    return unicodedata.normalize("NFKC", s or "")

BRAND_PATTERNS = [
    (re.compile(r"(æ—¥ç”£|ãƒ‹ãƒƒã‚µãƒ³|NISSAN)", re.IGNORECASE), "ãƒ‹ãƒƒã‚µãƒ³"),
    (re.compile(r"(ãƒˆãƒ¨ã‚¿|TOYOTA)", re.IGNORECASE), "ãƒˆãƒ¨ã‚¿"),
    (re.compile(r"(ãƒ›ãƒ³ãƒ€|HONDA)", re.IGNORECASE), "ãƒ›ãƒ³ãƒ€"),
    (re.compile(r"(ã‚¹ãƒãƒ«|SUBARU)", re.IGNORECASE), "ã‚¹ãƒãƒ«"),
    (re.compile(r"(ãƒãƒ„ãƒ€|MAZDA)", re.IGNORECASE), "ãƒãƒ„ãƒ€"),
    (re.compile(r"(ã‚¹ã‚ºã‚­|SUZUKI)", re.IGNORECASE), "ã‚¹ã‚ºã‚­"),
    (re.compile(r"(ä¸‰è±|ãƒŸãƒ„ãƒ“ã‚·|MITSUBISHI)", re.IGNORECASE), "ãƒŸãƒ„ãƒ“ã‚·"),
    (re.compile(r"(ãƒ€ã‚¤ãƒãƒ„|DAIHATSU)", re.IGNORECASE), "ãƒ€ã‚¤ãƒãƒ„"),
]
NISSAN_MODELS = ["ãƒªãƒ¼ãƒ•","ã‚»ãƒ¬ãƒŠ","ã‚¹ã‚«ã‚¤ãƒ©ã‚¤ãƒ³","ãƒ•ã‚§ã‚¢ãƒ¬ãƒ‡ã‚£Z","ãƒãƒ¼ãƒˆ","ã‚ªãƒ¼ãƒ©","ã‚¢ãƒªã‚¢","ã‚­ãƒƒã‚¯ã‚¹","ã‚¨ã‚¯ã‚¹ãƒˆãƒ¬ã‚¤ãƒ«","ã‚¸ãƒ¥ãƒ¼ã‚¯","ãƒ‡ã‚¤ã‚º","ãƒ«ãƒ¼ã‚¯ã‚¹","ãƒãƒ¼ãƒ","ãƒ†ã‚£ã‚¢ãƒŠ","ã‚·ãƒ«ãƒ“ã‚¢","GT-R"]
GEN_PREFIXES = [("æ–°å‹","æ–°å‹"),("ç¾è¡Œ","ç¾è¡Œ"),("æ—§å‹","æ—§å‹"),("å…ˆä»£","æ—§å‹")]

def detect_brand_name(title: str):
    for pat, name in BRAND_PATTERNS:
        if pat.search(title):
            return name
    return None

def detect_nissan_model(title: str):
    t = _norm(title)
    for m in NISSAN_MODELS:
        if _norm(m) in t:
            return m
    return None

def build_car_category(title: str):
    model = detect_nissan_model(title)
    if model:
        prefix = ""
        for key, norm in GEN_PREFIXES:
            if key in title:
                prefix = norm
                break
        label = f"{prefix}{model}" if prefix else model
        return f"è»Šï¼ˆ{label}ï¼‰"
    return None

def build_company_category(title: str):
    brand = detect_brand_name(title)
    return f"ä¼šç¤¾ï¼ˆ{brand if brand else 'ãã®ä»–'}ï¼‰"

def normalize_category(title: str, gemini_cat: str):
    t = _norm(title)
    base = (gemini_cat or "").strip()
    if detect_nissan_model(title):
        return build_car_category(title)
    if detect_brand_name(title):
        return build_company_category(title)
    if "EV" in t or "é›»æ°—è‡ªå‹•è»Š" in t:
        return "æŠ€è¡“ï¼ˆEVï¼‰"
    if "e-POWER" in t or "ã‚¤ãƒ¼ãƒ‘ãƒ¯ãƒ¼" in t:
        return "æŠ€è¡“ï¼ˆe-POWERï¼‰"
    if "e-4ORCE" in t or "AWD" in t:
        return "æŠ€è¡“ï¼ˆe-4ORCEï¼‰"
    if "è‡ªå‹•é‹è»¢" in t or "ADAS" in t:
        return "æŠ€è¡“ï¼ˆAD/ADASï¼‰"
    return base if base else "ãã®ä»–"

# ========= é›†ç´„ & å‡ºåŠ› =========
def build_daily_sheet(sh, all_items):
    now_jst = datetime.utcnow() + timedelta(hours=9)
    start, end, sheet_label = compute_window(now_jst)

    filtered = {"MSN": [], "Google": [], "Yahoo": []}
    no_date = 0
    for src, link, title, pubdate_raw, site in all_items:
        dt = parse_pubdate(pubdate_raw)
        if not dt:
            no_date += 1
            continue
        dt_jst = dt + timedelta(hours=9)
        if not (start <= dt_jst <= end):
            continue
        filtered[src].append((src, link, title, dt_jst.strftime("%Y/%m/%d %H:%M"), site))

    all_rows = filtered["MSN"] + filtered["Google"] + filtered["Yahoo"]

    print(f"ğŸ•’ é›†ç´„æœŸé–“: {start} ã€œ {end} â†’ ã‚·ãƒ¼ãƒˆå: {sheet_label}")
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿çµæœ: MSN={len(filtered['MSN'])}, Google={len(filtered['Google'])}, Yahoo={len(filtered['Yahoo'])}, æ—¥ä»˜ç„¡ã—ã‚¹ã‚­ãƒƒãƒ—={no_date}")

    ws = None
    try:
        ws = sh.worksheet(sheet_label)
        ws.clear()
    except Exception:
        ws = sh.add_worksheet(title=sheet_label, rows="1000", cols="20")

    ws.update("A1", [["ã‚½ãƒ¼ã‚¹","URL","ã‚¿ã‚¤ãƒˆãƒ«","æŠ•ç¨¿æ—¥","å¼•ç”¨å…ƒ","ã‚³ãƒ¡ãƒ³ãƒˆæ•°","ãƒã‚¸ãƒã‚¬","ã‚«ãƒ†ã‚´ãƒª"]])

    titles = [r[2] for r in all_rows]
    gemini_map = analyze_titles_gemini(titles)

    rows = []
    for src, link, title, date_str, site in all_rows:
        sentiment, category = gemini_map.get(title, ("", ""))
        category = normalize_category(title, category)
        rows.append([src, link, title, date_str, site, "", sentiment, category])

    if rows:
        ws.update(f"A2", rows)

    print(f"âœ… é›†ç´„ã‚·ãƒ¼ãƒˆ {sheet_label}: {len(rows)} ä»¶")
    return sheet_label

# ========= ãƒ¡ã‚¤ãƒ³ =========
def main():
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {NEWS_KEYWORD}")
    print(f"ğŸ“„ SPREADSHEET_ID: {SPREADSHEET_ID}")
    sh = get_spreadsheet(SPREADSHEET_ID)
    print(f"ğŸ“˜ Opened spreadsheet title: {sh.title}")

    google_items = fetch_google_news(NEWS_KEYWORD)
    yahoo_items = fetch_yahoo_news(NEWS_KEYWORD)
    msn_items = fetch_msn_news(NEWS_KEYWORD)

    print(f"--- å–å¾— ---")
    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(google_items)} ä»¶")
    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(yahoo_items)} ä»¶")
    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(msn_items)} ä»¶")

    all_items = google_items + yahoo_items + msn_items
    build_daily_sheet(sh, all_items)

if __name__ == "__main__":
    main()
