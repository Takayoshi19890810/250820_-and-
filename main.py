# -*- coding: utf-8 -*-
"""
ã¾ã¨ã‚ã‚·ãƒ¼ãƒˆã®ã¿å‡ºåŠ› / ä»Šæ—¥ã® YYMMDD ã«ã€æ˜¨æ—¥15:00ã€œä»Šæ—¥14:59 ã®è¨˜äº‹ã‚’é›†ç´„
+ Gemini ã‚’ã€Œãƒãƒƒãƒæ¨è«–ã€ã§ä½¿ç”¨ã—ã€Cåˆ—ã‚¿ã‚¤ãƒˆãƒ« â†’ Gåˆ—(ãƒã‚¸/ãƒã‚¬/ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«)ãƒ»Håˆ—(ã‚«ãƒ†ã‚´ãƒª) ã‚’ä¸€æ‹¬ä»˜ä¸
+ Yahoo è¨˜äº‹ã®ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—ã—ã¦ Fåˆ—ã«è¨˜è¼‰ï¼ˆ/comments?page=N ã‚’ Selenium ã§å·¡å›ã—ã¦æ•°ãˆã‚‹ï¼‰
"""

import os
import re
import json
import time
import requests
from datetime import datetime, timedelta, time as dtime
from email.utils import parsedate_to_datetime

import gspread
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# === Gemini ===
import google.generativeai as genai

# ====== è¨­å®š ======
KEYWORD = os.getenv("NEWS_KEYWORD", "æ—¥ç”£")
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID", "1Vs4Cx8QPN4H2NOgtwaviOCe8zBTpUNDgJjqkHr51IZE")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")

# ====== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ======
def jst_now() -> datetime:
    return datetime.utcnow() + timedelta(hours=9)

def fmt(dt: datetime) -> str:
    return dt.strftime("%Y/%m/%d %H:%M")

def try_parse_jst(dt_str: str):
    if not dt_str or dt_str == "å–å¾—ä¸å¯":
        return None
    patterns = [
        "%Y/%m/%d %H:%M", "%Y/%m/%d %H:%M:%S", "%Y/%m/%d",
        "%Y-%m-%d %H:%M",
        "%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%dT%H:%M:%S.%fZ", "%Y-%m-%dT%H:%M:%S.%f%z",
    ]
    for p in patterns:
        try:
            dt = datetime.strptime(dt_str, p)
            if p.endswith("Z"):
                dt = dt + timedelta(hours=9)
            elif "%z" in p:
                dt = dt.astimezone(tz=None); dt = dt + timedelta(hours=9)
            return dt
        except Exception:
            pass
    return None

def parse_relative_time(label: str, base: datetime) -> str:
    s = (label or "").strip()
    try:
        m = re.search(r"(\d+)\s*åˆ†å‰", s)
        if m: return fmt(base - timedelta(minutes=int(m.group(1))))
        h = re.search(r"(\d+)\s*æ™‚é–“å‰", s)
        if h: return fmt(base - timedelta(hours=int(h.group(1))))
        d = re.search(r"(\d+)\s*æ—¥å‰", s)
        if d: return fmt(base - timedelta(days=int(d.group(1))))
        if re.match(r"\d{1,2}æœˆ\d{1,2}æ—¥$", s):
            dt = datetime.strptime(f"{base.year}å¹´{s}", "%Yå¹´%mæœˆ%dæ—¥")
            return fmt(dt)
        if re.match(r"\d{4}/\d{1,2}/\d{1,2}$", s):
            dt = datetime.strptime(s, "%Y/%m/%d")
            return fmt(dt)
        if re.match(r"\d{1,2}:\d{2}$", s):
            t = datetime.strptime(s, "%H:%M").time()
            dt = datetime.combine(base.date(), t)
            if dt > base: dt -= timedelta(days=1)
            return fmt(dt)
    except Exception:
        pass
    return "å–å¾—ä¸å¯"

def get_last_modified_datetime(url: str) -> str:
    try:
        r = requests.head(url, timeout=6, allow_redirects=True)
        if "Last-Modified" in r.headers:
            dt = parsedate_to_datetime(r.headers["Last-Modified"])
            if dt.tzinfo:
                dt = dt.astimezone(tz=None); dt = dt + timedelta(hours=9)
            else:
                dt = dt + timedelta(hours=9)
            return fmt(dt)
    except Exception:
        pass
    return "å–å¾—ä¸å¯"

def fetch_html(url: str, timeout: int = 10):
    try:
        hdrs = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=hdrs, timeout=timeout)
        if r.ok: return r.text
    except Exception:
        pass
    return ""

def extract_datetime_from_article(html: str) -> str:
    if not html: return "å–å¾—ä¸å¯"
    soup = BeautifulSoup(html, "html.parser")
    # JSON-LD
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(tag.string or "{}")
            objs = data if isinstance(data, list) else [data]
            for obj in objs:
                if not isinstance(obj, dict): continue
                for key in ["datePublished", "dateModified", "uploadDate"]:
                    if obj.get(key):
                        dt = try_parse_jst(str(obj[key]).strip())
                        if dt: return fmt(dt)
        except Exception:
            continue
    # <time datetime>
    t = soup.find("time", attrs={"datetime": True})
    if t and t.get("datetime"):
        dt = try_parse_jst(t["datetime"].strip())
        if dt: return fmt(dt)
    # OG
    for prop in ["article:published_time", "article:modified_time", "og:updated_time"]:
        m = soup.find("meta", attrs={"property": prop, "content": True})
        if m and m.get("content"):
            dt = try_parse_jst(m["content"].strip())
            if dt: return fmt(dt)
    return "å–å¾—ä¸å¯"

def extract_title_and_source_from_yahoo(html: str):
    title, source = "", "Yahoo"
    if not html: return title, source
    soup = BeautifulSoup(html, "html.parser")
    # JSON-LD
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(tag.string or "{}")
            objs = data if isinstance(data, list) else [data]
            for obj in objs:
                if isinstance(obj, dict):
                    if not title and obj.get("headline"):
                        title = str(obj["headline"]).strip()
                    if source == "Yahoo":
                        pub = obj.get("publisher")
                        if isinstance(pub, dict) and pub.get("name"):
                            source = str(pub["name"]).strip() or "Yahoo"
        except Exception:
            continue
    # <h1> / twitter:title / og:title
    if not title:
        h1 = soup.find("h1")
        if h1: title = h1.get_text(strip=True)
    if not title:
        tw = soup.find("meta", attrs={"name": "twitter:title", "content": True})
        if tw and tw["content"].strip() != "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹":
            title = tw["content"].strip()
    if not title:
        og = soup.find("meta", attrs={"property": "og:title", "content": True})
        if og and og["content"].strip() != "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹":
            title = og["content"].strip()
    # å‡ºå…¸
    if source == "Yahoo":
        src_meta = soup.find("meta", attrs={"name": "source", "content": True})
        if src_meta and src_meta.get("content"):
            source = src_meta["content"].strip() or "Yahoo"
    if source == "Yahoo":
        cand = soup.find(["span","div"], string=True)
        if cand:
            txt = cand.get_text(strip=True)
            if 2 <= len(txt) <= 30 and not txt.isdigit():
                source = txt
    return title, source

def resolve_yahoo_article_url(html: str, orig_url: str) -> str:
    if not html:
        return orig_url
    soup = BeautifulSoup(html, "html.parser")
    a = soup.select_one('a[href*="news.yahoo.co.jp/articles/"]')
    if a and a.get("href"):
        return a["href"]
    can = soup.find("link", rel="canonical")
    if can and can.get("href"):
        return can["href"]
    return orig_url

def chrome_driver():
    opt = Options()
    opt.add_argument("--headless=new")
    opt.add_argument("--disable-gpu")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-dev-shm-usage")
    opt.add_argument("--window-size=1920,1080")
    opt.add_argument("--lang=ja-JP")
    # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹æ¤œçŸ¥ã‚’å¼±ã‚ã‚‹
    opt.add_argument("--disable-blink-features=AutomationControlled")
    opt.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
    )
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opt)
    try:
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        })
    except Exception:
        pass
    return driver

# ====== å–å¾—ï¼šGoogle ======
def get_google_news(keyword: str):
    driver = chrome_driver()
    url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    driver.get(url)
    time.sleep(5)
    for _ in range(4):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.2)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data, with_time = [], 0
    for art in soup.find_all("article"):
        try:
            a_tag = art.select_one("a.JtKRv") or art.select_one("a.WwrzSb") or art.select_one("a.DY5T1d") or art.select_one("h3 a")
            time_el = art.select_one("time[datetime]") or art.find("time")
            src_el = art.select_one("div.vr1PYe") or art.select_one("div.SVJrMe")
            if not a_tag: continue
            title = a_tag.get_text(strip=True)
            href  = a_tag.get("href")
            if not title or not href: continue
            url = "https://news.google.com" + href[1:] if href.startswith("./") else href
            if not url.startswith("http"): continue
            pub = "å–å¾—ä¸å¯"
            if time_el and time_el.get("datetime"):
                dt = try_parse_jst(time_el.get("datetime").strip())
                if dt: pub = fmt(dt); with_time += 1
            if pub == "å–å¾—ä¸å¯":
                html = fetch_html(url)
                pub = extract_datetime_from_article(html)
            source = (src_el.get_text(strip=True) if src_el else "Google") or "Google"
            data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": pub, "å¼•ç”¨å…ƒ": source, "ã‚½ãƒ¼ã‚¹": "Google"})
        except Exception:
            continue
    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(data)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾— {with_time} ä»¶ï¼‰")
    return data

# ====== å–å¾—ï¼šYahoo ======
def get_yahoo_news(keyword: str):
    driver = chrome_driver()
    url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(url)
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    def is_article(u: str) -> bool:
        return u and u.startswith("http") and ("news.yahoo.co.jp/articles/" in u or "news.yahoo.co.jp/pickup/" in u)

    raw_links = [a.get("href") for a in soup.find_all("a", href=True)]
    article_links = [u for u in raw_links if is_article(u)]

    data, with_time = [], 0
    seen = set()
    for href in article_links:
        if href in seen: continue
        seen.add(href)
        try:
            html0 = fetch_html(href)
            real_url = resolve_yahoo_article_url(html0, href)  # pickupâ†’è¨˜äº‹ã¸è§£æ±º
            html = fetch_html(real_url) if real_url != href else html0
            if not html: continue

            title, source = extract_title_and_source_from_yahoo(html)
            pub = extract_datetime_from_article(html)

            # ã‚¿ã‚¤ãƒˆãƒ«æœ€ä½é™ã‚¬ãƒ¼ãƒ‰
            if not title or title == "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹":
                continue
            if pub != "å–å¾—ä¸å¯": with_time += 1

            data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": real_url, "æŠ•ç¨¿æ—¥": pub, "å¼•ç”¨å…ƒ": source, "ã‚½ãƒ¼ã‚¹": "Yahoo"})
        except Exception:
            continue

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(data)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾— {with_time} ä»¶ï¼‰")
    return data

# ====== å–å¾—ï¼šMSNï¼ˆBing Newsï¼‰ ======
def get_msn_news(keyword: str):
    base = jst_now()
    driver = chrome_driver()
    url = ("https://www.bing.com/news/search"
           f"?q={keyword}"
           "&qft=sortbydate%3d'1'&setlang=ja-JP&mkt=ja-JP&cc=JP&form=YFNR")
    driver.get(url)
    time.sleep(5)
    for _ in range(4):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.2)
    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data, with_time = [], 0

    cards = soup.select("div.news-card[data-title][data-url]") or []
    for c in cards:
        try:
            title = (c.get("data-title") or "").strip()
            link  = (c.get("data-url") or "").strip()
            source = (c.get("data-author") or "").strip() or "MSN"
            if not title or not link.startswith("http"):
                continue
            lab = ""
            s = c.find("span", attrs={"aria-label": True})
            if s and s.has_attr("aria-label"):
                lab = s["aria-label"].strip()
            pub = parse_relative_time(lab, base)
            if pub == "å–å¾—ä¸å¯":
                html = fetch_html(link)
                pub = extract_datetime_from_article(html)
                if pub == "å–å¾—ä¸å¯":
                    pub = get_last_modified_datetime(link)
            else:
                with_time += 1
            data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": link, "æŠ•ç¨¿æ—¥": pub, "å¼•ç”¨å…ƒ": source, "ã‚½ãƒ¼ã‚¹": "MSN"})
        except Exception:
            continue

    if not data:
        items = soup.select("a.title, h2 a, h3 a, a[href*='/news/']")
        for a in items:
            try:
                href = a.get("href"); title = a.get_text(strip=True)
                if not href or not href.startswith("http") or not title:
                    continue
                cont = a.find_parent(["div","li","article"]) or soup
                lab = ""
                s = cont.find("span", attrs={"aria-label": True})
                if s and s.has_attr("aria-label"): lab = s["aria-label"].strip()
                pub = parse_relative_time(lab, base)
                if pub == "å–å¾—ä¸å¯":
                    html = fetch_html(href)
                    pub = extract_datetime_from_article(html)
                    if pub == "å–å¾—ä¸å¯":
                        pub = get_last_modified_datetime(href)
                else:
                    with_time += 1
                source = "MSN"
                src_el = cont.find(["span","div"], class_=re.compile("source|provider"))
                if src_el:
                    st = src_el.get_text(strip=True)
                    if st: source = st
                data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": href, "æŠ•ç¨¿æ—¥": pub, "å¼•ç”¨å…ƒ": source, "ã‚½ãƒ¼ã‚¹": "MSN"})
            except Exception:
                continue

    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(data)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾—/æ¨å®š {with_time} ä»¶ï¼‰")
    return data

# ====== Yahoo ã‚³ãƒ¡ãƒ³ãƒˆæ•° ======
def count_yahoo_comments_with_driver(driver, url: str, max_pages: int = 10, sleep_sec: float = 2.0) -> int:
    """
    Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã«å¯¾ã— /comments?page=N ã‚’é–‹ã„ã¦ <p class='sc-169yn8p-10'> ã‚’æ•°ãˆã‚‹æ–¹å¼ã€‚
    å‚ç…§ã„ãŸã ã„ãŸã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç°¡ç•¥åŒ–ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆå°‚ç”¨ã«ã—ã¦ã„ã¾ã™ã€‚
    """
    total = 0
    prev_first = None
    for page in range(1, max_pages + 1):
        c_url = f"{url.rstrip('/')}/comments?page={page}"
        try:
            driver.get(c_url)
            time.sleep(sleep_sec)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            elems = soup.find_all("p", class_="sc-169yn8p-10")
            if not elems:
                break
            first_text = elems[0].get_text(strip=True) if elems else None
            # åŒã˜å†…å®¹ãŒãƒ«ãƒ¼ãƒ—ã—å§‹ã‚ãŸã‚‰çµ‚äº†
            if prev_first and first_text == prev_first:
                break
            prev_first = first_text
            total += len(elems)
        except Exception:
            break
    return total

def get_yahoo_comment_counts(urls: list, sleep_sec: float = 2.0) -> dict:
    """
    è¤‡æ•°URLã‚’1ã¤ã®ãƒ‰ãƒ©ã‚¤ãƒã§é †ã«ã‚«ã‚¦ãƒ³ãƒˆã—ã¦ã€{url: count} ã‚’è¿”ã™
    """
    if not urls:
        return {}
    driver = chrome_driver()
    out = {}
    try:
        for u in urls:
            try:
                out[u] = count_yahoo_comments_with_driver(driver, u, sleep_sec=sleep_sec)
            except Exception:
                out[u] = 0
    finally:
        driver.quit()
    return out

# ====== ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ ======
def get_gspread_client():
    key = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
    if key:
        try:
            creds = json.loads(key)
            return gspread.service_account_from_dict(creds)
        except Exception as e:
            raise RuntimeError(f"GCP_SERVICE_ACCOUNT_KEY ã®JSONãŒä¸æ­£ã§ã™: {e}")
    return gspread.service_account(filename="credentials.json")

def compute_window(now_jst: datetime):
    today = now_jst.date()
    today_1500 = datetime.combine(today, dtime(hour=15, minute=0))
    start = today_1500 - timedelta(days=1)        # æ˜¨æ—¥15:00
    end   = today_1500 - timedelta(seconds=1)     # ä»Šæ—¥14:59:59
    label = today.strftime("%y%m%d")              # ä»Šæ—¥ â†’ YYMMDD
    return start, end, label

def build_daily_sheet(sh, rows_all: list):
    now = jst_now()
    start, end, label = compute_window(now)
    print(f"ğŸ•’ é›†ç´„æœŸé–“: {fmt(start)} ã€œ {fmt(end)} â†’ ã‚·ãƒ¼ãƒˆå: {label}")

    filtered = {"MSN": [], "Google": [], "Yahoo": []}
    no_date = 0
    for r in rows_all:
        dt = try_parse_jst(r.get("æŠ•ç¨¿æ—¥", ""))
        if not dt:
            no_date += 1
            continue
        if start <= dt <= end:
            src = r.get("ã‚½ãƒ¼ã‚¹","")
            if src in filtered:
                filtered[src].append(r)

    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿çµæœ: MSN={len(filtered['MSN'])}, Google={len(filtered['Google'])}, Yahoo={len(filtered['Yahoo'])}, æ—¥ä»˜ç„¡ã—ã‚¹ã‚­ãƒƒãƒ—={no_date}")

    def dedup_sort(lst):
        seen = set(); uniq = []
        for d in lst:
            if d["URL"] not in seen:
                seen.add(d["URL"]); uniq.append(d)
        uniq.sort(key=lambda x: try_parse_jst(x["æŠ•ç¨¿æ—¥"]) or datetime(1970,1,1), reverse=True)
        return uniq

    ordered = []
    for src in ["MSN", "Google", "Yahoo"]:
        ordered.extend(dedup_sort(filtered[src]))

    # --- Yahooã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’ä¸€æ‹¬å–å¾— ---
    yahoo_urls = [d["URL"] for d in ordered if d.get("ã‚½ãƒ¼ã‚¹") == "Yahoo"]
    cmt_map = get_yahoo_comment_counts(sorted(set(yahoo_urls))) if yahoo_urls else {}

    # ãƒ˜ãƒƒãƒ€ãƒ¼: A..F ã¾ã§ä½¿ç”¨ï¼ˆG/H ã¯ Gemini ç”¨ï¼‰
    headers = ["ã‚½ãƒ¼ã‚¹", "URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°"]  # A..F
    try:
        ws = sh.worksheet(label)
        ws.clear(); ws.append_row(headers)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=label, rows=str(max(2, len(ordered)+5)), cols="9")
        ws.append_row(headers)

    if ordered:
        rows = []
        for d in ordered:
            cnt = ""
            if d["ã‚½ãƒ¼ã‚¹"] == "Yahoo":
                cnt = cmt_map.get(d["URL"], 0)
            rows.append([d["ã‚½ãƒ¼ã‚¹"], d["URL"], d["ã‚¿ã‚¤ãƒˆãƒ«"], d["æŠ•ç¨¿æ—¥"], d["å¼•ç”¨å…ƒ"], cnt])
        ws.append_rows(rows, value_input_option="USER_ENTERED")
        print(f"âœ… é›†ç´„ã‚·ãƒ¼ãƒˆ {label}: {len(rows)} ä»¶ï¼ˆYahooã‚³ãƒ¡ãƒ³ãƒˆæ•° ä»˜ä¸: {len(yahoo_urls)} ä»¶ï¼‰")
    else:
        print(f"âš ï¸ é›†ç´„ã‚·ãƒ¼ãƒˆ {label}: å¯¾è±¡è¨˜äº‹ãªã—")

    # G/H ãƒ˜ãƒƒãƒ€ãƒ¼
    ws.update("G1:H1", [["ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª"]])

    return label

# ====== Gemini ãƒãƒƒãƒåˆ†é¡ ======
GEMINI_SYSTEM_PROMPT = """ã‚ãªãŸã¯æ•è…•é›‘èªŒè¨˜è€…ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸã€Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã€ä¸€è¦§ã«ã¤ã„ã¦ã€
å„ã‚¿ã‚¤ãƒˆãƒ«ã”ã¨ã«ä»¥ä¸‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
â‘ ãƒã‚¸ãƒ†ã‚£ãƒ–ï¼ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ« ã®ã„ãšã‚Œã‹1ã¤
â‘¡ã‚«ãƒ†ã‚´ãƒªï¼ˆä»¥ä¸‹ã®ä¸­ã‹ã‚‰æœ€ã‚‚é–¢é€£æ€§ãŒé«˜ã„1ã¤ã ã‘ï¼‰ï¼š
ä¼šç¤¾ã€è»Šã€è»Šï¼ˆç«¶åˆï¼‰ã€æŠ€è¡“ï¼ˆEVï¼‰ã€æŠ€è¡“ï¼ˆe-POWERï¼‰ã€æŠ€è¡“ï¼ˆe-4ORCEï¼‰ã€æŠ€è¡“ï¼ˆAD/ADASï¼‰ã€æŠ€è¡“ã€ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚¹ãƒãƒ¼ãƒ„ã€æ ªå¼ã€æ”¿æ²»ãƒ»çµŒæ¸ˆã€ã‚¹ãƒãƒ¼ãƒ„ã€ãã®ä»–

è¿½åŠ ãƒ«ãƒ¼ãƒ«ï¼š
- ã€Œä¼šç¤¾ã€ï¼šãƒ‹ãƒƒã‚µãƒ³ã€ãƒˆãƒ¨ã‚¿ã€ãƒ›ãƒ³ãƒ€ã€ã‚¹ãƒãƒ«ã€ãƒãƒ„ãƒ€ã€ã‚¹ã‚ºã‚­ã€ãƒŸãƒ„ãƒ“ã‚·ã€ãƒ€ã‚¤ãƒãƒ„ã®è¨˜äº‹ã¯ () ã«ä¼æ¥­åã€‚ãã®ä»–ã¯ã€Œãã®ä»–ã€ã€‚
- ã€Œè»Šã€ï¼šè»ŠåãŒå«ã¾ã‚Œã‚‹å ´åˆã®ã¿ï¼ˆä¼šç¤¾åã ã‘ã¯ä¸å¯ï¼‰ã€‚æ–°å‹/ç¾è¡Œ/æ—§å‹ + åç§°ã‚’ () ä»˜ã§è¨˜è¼‰ï¼ˆä¾‹ï¼šæ–°å‹ãƒªãƒ¼ãƒ•ã€ç¾è¡Œã‚»ãƒ¬ãƒŠã€æ—§å‹ã‚¹ã‚«ã‚¤ãƒ©ã‚¤ãƒ³ï¼‰ã€‚æ—¥ç”£ä»¥å¤–ã¯ã€Œè»Šï¼ˆç«¶åˆï¼‰ã€ã€‚
- æŠ€è¡“ï¼ˆEV / e-POWER / e-4ORCE / AD/ADASï¼‰ï¼šè©²å½“ã™ã‚Œã°ãã‚Œã‚’å„ªå…ˆã€‚ãã®ä»–ã®æŠ€è¡“ã¯ã€ŒæŠ€è¡“ã€ã€‚
- å‡ºåŠ›ã¯ JSONé…åˆ—ã§ã€å„è¦ç´ ã¯ {"row": æ•°å€¤, "sentiment":"ãƒã‚¸ãƒ†ã‚£ãƒ–|ãƒã‚¬ãƒ†ã‚£ãƒ–|ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "category":"..."} ã®å½¢å¼ã€‚è¡Œç•ªå· row ã¯ä¸ãˆãŸIDã‚’ãã®ã¾ã¾è¿”ã™ã“ã¨ã€‚
- ã‚¿ã‚¤ãƒˆãƒ«æ–‡è¨€ã¯æ”¹å¤‰ã—ãªã„ã“ã¨ã€‚
"""

def setup_gemini():
    if not GEMINI_API_KEY:
        print("âš ï¸ GEMINI_API_KEY ãŒæœªè¨­å®šã®ãŸã‚ã€åˆ†é¡ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None
    genai.configure(api_key=GEMINI_API_KEY)
    return genai.GenerativeModel("gemini-1.5-flash")

def build_batch_prompt(tuples):
    data = [{"row": r, "title": t} for (r, t) in tuples]
    payload = json.dumps(data, ensure_ascii=False)
    prompt = GEMINI_SYSTEM_PROMPT + "\n\nãƒ‡ãƒ¼ã‚¿:\n" + payload + "\n\nä¸Šè¨˜ã«å¯¾ã™ã‚‹å›ç­”ã®ã¿ã‚’JSONé…åˆ—ã§è¿”ã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"
    return prompt

def parse_batch_response(text):
    if not text:
        return []
    m = re.search(r"\[\s*\{.*\}\s*\]", text, re.S)
    if m:
        try:
            arr = json.loads(m.group(0))
            if isinstance(arr, list):
                return arr
        except Exception:
            pass
    m2 = re.search(r"\{.*\}", text, re.S)
    if m2:
        try:
            obj = json.loads(m2.group(0))
            if isinstance(obj, dict):
                return [obj]
        except Exception:
            pass
    return []

def classify_titles_in_batches(sh, sheet_name: str, batch_size: int = 80, sleep_sec: float = 0.5):
    model = setup_gemini()
    if model is None:
        return

    ws = sh.worksheet(sheet_name)
    values = ws.get_all_values()
    if not values or len(values[0]) < 3:
        print("âš ï¸ ã‚¿ã‚¤ãƒˆãƒ«åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # 2è¡Œç›®ä»¥é™ã® Cåˆ—ã‚’ã¾ã¨ã‚ã¦åˆ¤å®š
    items = []
    for idx, row in enumerate(values[1:], start=2):
        title = row[2] if len(row) > 2 else ""
        if title:
            items.append((idx, title))

    if not items:
        print("âš ï¸ Geminiåˆ†é¡å¯¾è±¡ãªã—ã€‚"); return

    results_map = {}

    for i in range(0, len(items), batch_size):
        batch = items[i:i+batch_size]
        prompt = build_batch_prompt(batch)
        try:
            resp = model.generate_content(prompt)
            text = (getattr(resp, "text", "") or "").strip()
        except Exception as e:
            print(f"Geminiãƒãƒƒãƒå¤±æ•—: {e}")
            for r,_ in batch:
                results_map[r] = ("ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "ãã®ä»–")
            time.sleep(sleep_sec)
            continue

        arr = parse_batch_response(text)
        if not arr:
            for r,_ in batch:
                results_map[r] = ("ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "ãã®ä»–")
        else:
            covered = set()
            for obj in arr:
                try:
                    r = int(obj.get("row"))
                    s = str(obj.get("sentiment","")).strip() or "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«"
                    c = str(obj.get("category","")).strip() or "ãã®ä»–"
                    results_map[r] = (s, c)
                    covered.add(r)
                except Exception:
                    continue
            for (r, _) in batch:
                if r not in covered:
                    results_map[r] = ("ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "ãã®ä»–")

        time.sleep(sleep_sec)

    # ä¸€æ‹¬æ›¸ãè¾¼ã¿ï¼ˆG/Hï¼‰
    updates = []
    min_row = 2
    max_row = max(results_map.keys()) if results_map else 1
    for r in range(min_row, max_row + 1):
        if r in results_map:
            s, c = results_map[r]
        else:
            s, c = ("", "")
        updates.append([s, c])

    if updates:
        ws.update(f"G{min_row}:H{min_row + len(updates) - 1}", updates, value_input_option="USER_ENTERED")
        print(f"âœ… Geminiãƒãƒƒãƒåˆ†é¡å®Œäº†: {len(items)} ã‚¿ã‚¤ãƒˆãƒ« / å‘¼ã³å‡ºã— {((len(items)-1)//batch_size)+1} å›")
    else:
        print("âš ï¸ Geminiåˆ†é¡ã®æ›¸ãè¾¼ã¿å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

# ====== ãƒ¡ã‚¤ãƒ³ ======
def main():
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {KEYWORD}")
    print(f"ğŸ“„ SPREADSHEET_ID: {SPREADSHEET_ID}")

    gc = get_gspread_client()
    sh = gc.open_by_key(SPREADSHEET_ID)
    print(f"ğŸ“˜ Opened spreadsheet title: {sh.title}")

    print("\n--- å–å¾— ---")
    google_items = get_google_news(KEYWORD)
    yahoo_items  = get_yahoo_news(KEYWORD)
    msn_items    = get_msn_news(KEYWORD)

    all_items = []
    all_items.extend(msn_items)
    all_items.extend(google_items)
    all_items.extend(yahoo_items)

    print("\n--- é›†ç´„ï¼ˆã¾ã¨ã‚ã‚·ãƒ¼ãƒˆã®ã¿ / Aåˆ—=ã‚½ãƒ¼ã‚¹ / é †=MSNâ†’Googleâ†’Yahooï¼‰ ---")
    sheet_name = build_daily_sheet(sh, all_items)

    print("\n--- Geminiï¼ˆç„¡æ–™æ ç¯€ç´„ã®ãƒãƒƒãƒï¼‰ã§ãƒã‚¸/ãƒã‚¬ï¼†ã‚«ãƒ†ã‚´ãƒªä»˜ä¸ï¼ˆGåˆ—/Håˆ—ï¼‰ ---")
    classify_titles_in_batches(sh, sheet_name, batch_size=80, sleep_sec=0.5)

if __name__ == "__main__":
    main()
