# -*- coding: utf-8 -*-
import os
import re
import json
import time
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime

import gspread
from google.oauth2.service_account import Credentials
import google.generativeai as genai

# ================== è¨­å®š ==================
NEWS_KEYWORD = os.environ.get("NEWS_KEYWORD", "æ—¥ç”£")
SPREADSHEET_ID = os.environ.get("SPREADSHEET_ID")            # å¿…é ˆ
GCP_SERVICE_ACCOUNT_KEY = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")  # å¿…é ˆ(JSON)
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")            # ä»»æ„ï¼ˆæœªè¨­å®šãªã‚‰åˆ†é¡ã¯ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ï¼‰

JST = timezone(timedelta(hours=9))

def now_jst() -> datetime:
    return datetime.now(JST)

def fmt_jst(dt: datetime) -> str:
    return dt.astimezone(JST).strftime("%Y/%m/%d %H:%M")

UA = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    )
}

# ================== Google Sheets èªè¨¼ ==================
def get_gspread_client():
    if not GCP_SERVICE_ACCOUNT_KEY:
        raise RuntimeError("GCP_SERVICE_ACCOUNT_KEY ãŒæœªè¨­å®šã§ã™ã€‚")
    creds_json = json.loads(GCP_SERVICE_ACCOUNT_KEY)
    scopes = ["https://www.googleapis.com/auth/spreadsheets"]
    credentials = Credentials.from_service_account_info(creds_json, scopes=scopes)
    return gspread.authorize(credentials)

# ================== å…±é€šï¼šHTMLå–å¾— ==================
def fetch_html(url: str, timeout: int = 15) -> str:
    try:
        r = requests.get(url, headers=UA, timeout=timeout)
        if r.ok:
            r.encoding = r.apparent_encoding or "utf-8"
            return r.text
    except Exception:
        pass
    return ""

# ================== æ—¥ä»˜ãƒ˜ãƒ«ãƒ‘ ==================
def try_parse_jst(dt_str: str):
    patterns = [
        "%Y/%m/%d %H:%M", "%Y/%m/%d %H:%M:%S", "%Y/%m/%d",
        "%Y-%m-%d %H:%M", "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%dT%H:%M:%S.%fZ",
        "%Y-%m-%dT%H:%M:%S.%f%z",
    ]
    for p in patterns:
        try:
            d = datetime.strptime(dt_str, p)
            if p.endswith("Z"):
                d = d.replace(tzinfo=timezone.utc).astimezone(JST)
            elif "%z" in p:
                d = d.astimezone(JST)
            else:
                d = d.replace(tzinfo=JST)
            return d
        except Exception:
            pass
    return None

def extract_datetime_from_article(html: str) -> str:
    if not html:
        return ""
    soup = BeautifulSoup(html, "html.parser")
    # JSON-LD
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(tag.string or "{}")
            objs = data if isinstance(data, list) else [data]
            for obj in objs:
                if not isinstance(obj, dict):
                    continue
                for key in ("datePublished", "dateModified", "uploadDate"):
                    if obj.get(key):
                        dt = try_parse_jst(str(obj[key]).strip())
                        if dt:
                            return fmt_jst(dt)
        except Exception:
            continue
    # <time datetime>
    t = soup.find("time", attrs={"datetime": True})
    if t and t.get("datetime"):
        dt = try_parse_jst(t["datetime"].strip())
        if dt:
            return fmt_jst(dt)
    # OGP
    for prop in ("article:published_time", "article:modified_time", "og:updated_time"):
        m = soup.find("meta", attrs={"property": prop, "content": True})
        if m and m.get("content"):
            dt = try_parse_jst(m["content"].strip())
            if dt:
                return fmt_jst(dt)
    return ""

# ================== Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆRSSï¼‰ ==================
def fetch_google_news(keyword: str):
    url = f"https://news.google.com/rss/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    r = requests.get(url, headers=UA, timeout=15)
    r.raise_for_status()
    soup = None
    for parser in ("lxml-xml", "xml", "html.parser"):
        try:
            soup = BeautifulSoup(r.text, parser)
            if soup:
                break
        except Exception:
            soup = None
    if soup is None:
        return []
    items = []
    for it in soup.find_all("item"):
        title = (it.title.text if it.title else "").strip()
        link = (it.link.text if it.link else "").strip()
        src  = (it.source.text if it.source else "Google").strip()
        pub = ""
        if it.pubDate and it.pubDate.text:
            try:
                dt = parsedate_to_datetime(it.pubDate.text.strip())
                pub = fmt_jst(dt)
            except Exception:
                pub = ""
        if title and link:
            items.append(("Google", link, title, pub, src))
    return items

# ================== MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆç°¡æ˜“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ï¼‰ ==================
def fetch_msn_news(keyword: str):
    url = f"https://www.bing.com/news/search?q={keyword}&cc=jp"
    html = fetch_html(url)
    soup = BeautifulSoup(html, "html.parser")
    items = []
    for a in soup.select("a.title, h2 a, h3 a"):
        link = a.get("href") or ""
        title = a.get_text(strip=True)
        if not title or not link:
            continue
        pub = fmt_jst(now_jst())  # å–å¾—æ™‚åˆ»
        src = "MSN"
        items.append(("MSN", link, title, pub, src))
    return items

# ================== Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ¤œç´¢â†’è¨˜äº‹æŠ½å‡ºï¼‰â€»ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¯å–å¾—ã—ãªã„ ==================
def resolve_yahoo_article_url(html: str, fallback_url: str) -> str:
    if not html:
        return fallback_url
    soup = BeautifulSoup(html, "html.parser")
    # canonical
    can = soup.find("link", rel="canonical")
    if can and can.get("href"):
        href = can["href"]
        if "news.yahoo.co.jp/articles/" in href:
            return href
    a = soup.select_one('a[href*="news.yahoo.co.jp/articles/"]')
    if a and a.get("href"):
        return a["href"]
    return fallback_url

def extract_yahoo_title_source(html: str) -> tuple[str, str]:
    title, source = "", "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹"
    if not html:
        return title, source
    soup = BeautifulSoup(html, "html.parser")
    # JSON-LD
    for tag in soup.find_all("script", {"type": "application/ld+json"}):
        try:
            data = json.loads(tag.string or "{}")
            objs = data if isinstance(data, list) else [data]
            for obj in objs:
                if isinstance(obj, dict):
                    if not title and obj.get("headline"):
                        title = str(obj["headline"]).strip()
                    pub = obj.get("publisher")
                    if pub and isinstance(pub, dict) and pub.get("name"):
                        source = str(pub["name"]).strip() or source
        except Exception:
            continue
    # Fallback
    if not title:
        h1 = soup.find("h1")
        if h1:
            title = h1.get_text(strip=True)
    if not title:
        og = soup.find("meta", attrs={"property": "og:title", "content": True})
        if og and og.get("content"):
            t = og["content"].strip()
            if t and t != "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹":
                title = t
    return title, source

def fetch_yahoo_news(keyword: str):
    url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&ts=0&st=n&sr=1&sk=all"
    html = fetch_html(url)
    soup = BeautifulSoup(html, "html.parser")

    cand_urls = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "news.yahoo.co.jp/articles/" in href or "news.yahoo.co.jp/pickup/" in href:
            if href.startswith("//"):
                href = "https:" + href
            if href.startswith("http"):
                cand_urls.append(href)

    seen, targets = set(), []
    for u in cand_urls:
        if u not in seen:
            seen.add(u)
            targets.append(u)

    items = []
    for u in targets:
        try:
            html0 = fetch_html(u)
            art_url = resolve_yahoo_article_url(html0, u)
            if "news.yahoo.co.jp/pickup/" in art_url and art_url == u:
                continue  # pickupâ†’è¨˜äº‹æœªè§£æ±ºã¯ã‚¹ã‚­ãƒƒãƒ—
            html1 = html0 if art_url == u else fetch_html(art_url)
            if not html1:
                continue

            title, source = extract_yahoo_title_source(html1)
            if not title or title == "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹":
                og = BeautifulSoup(html1, "html.parser").find("meta", attrs={"property": "og:title", "content": True})
                if og and og.get("content"):
                    t = og["content"].strip()
                    if t and t != "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹":
                        title = t

            pub = extract_datetime_from_article(html1) or fmt_jst(now_jst())

            # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¯å–å¾—ã—ãªã„ â†’ ç©ºæ¬„
            comment = ""

            items.append(("Yahoo", art_url, title, pub, source, comment))
            time.sleep(0.2)
        except Exception:
            continue
    return items

# ================== ã‚«ãƒ†ã‚´ãƒªãƒ¼æ•´å½¢ï¼ˆå³æ ¼ãƒ«ãƒ¼ãƒ«ï¼‰ ==================
# ä¼æ¥­ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œå‡ºï¼ˆä¼šç¤¾ã‚«ãƒ†ã‚´ãƒªç”¨ï¼‰
BRAND_PATTERNS = [
    (re.compile(r"(æ—¥ç”£|ãƒ‹ãƒƒã‚µãƒ³|NISSAN)", re.IGNORECASE), "ãƒ‹ãƒƒã‚µãƒ³"),
    (re.compile(r"(ãƒˆãƒ¨ã‚¿|TOYOTA)", re.IGNORECASE), "ãƒˆãƒ¨ã‚¿"),
    (re.compile(r"(ãƒ›ãƒ³ãƒ€|HONDA)", re.IGNORECASE), "ãƒ›ãƒ³ãƒ€"),
    (re.compile(r"(ã‚¹ãƒãƒ«|SUBARU)", re.IGNORECASE), "ã‚¹ãƒãƒ«"),
    (re.compile(r"(ãƒãƒ„ãƒ€|MAZDA)", re.IGNORECASE), "ãƒãƒ„ãƒ€"),
    (re.compile(r"(ã‚¹ã‚ºã‚­|SUZUKI)", re.IGNORECASE), "ã‚¹ã‚ºã‚­"),
    (re.compile(r"(ä¸‰è±|ãƒŸãƒ„ãƒ“ã‚·|MITSUBISHI)", re.IGNORECASE), "ãƒŸãƒ„ãƒ“ã‚·"),
    (re.compile(r"(ãƒ€ã‚¤ãƒãƒ„|DAIHATSU)", re.IGNORECASE), "ãƒ€ã‚¤ãƒãƒ„"),
]

# ãƒ‹ãƒƒã‚µãƒ³è»Šåï¼ˆè»Šã‚«ãƒ†ã‚´ãƒªã®è¡¨è¨˜ç”¨ï¼‰
NISSAN_MODELS = [
    "ãƒªãƒ¼ãƒ•","ã‚»ãƒ¬ãƒŠ","ã‚¹ã‚«ã‚¤ãƒ©ã‚¤ãƒ³","ãƒ•ã‚§ã‚¢ãƒ¬ãƒ‡ã‚£Z","ãƒãƒ¼ãƒˆ","ã‚ªãƒ¼ãƒ©","ã‚¢ãƒªã‚¢","ã‚­ãƒƒã‚¯ã‚¹","ã‚¨ã‚¯ã‚¹ãƒˆãƒ¬ã‚¤ãƒ«",
    "ã‚¸ãƒ¥ãƒ¼ã‚¯","ãƒ‡ã‚¤ã‚º","ãƒ«ãƒ¼ã‚¯ã‚¹","ãƒãƒ¼ãƒ","ãƒ†ã‚£ã‚¢ãƒŠ","ã‚·ãƒ«ãƒ“ã‚¢","GT-R","R35","ã‚­ãƒ£ãƒ©ãƒãƒ³","ãƒ‘ãƒˆãƒ­ãƒ¼ãƒ«",
    "ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢","ãƒ‡ãƒ¥ã‚¢ãƒªã‚¹","ãƒ©ãƒ†ã‚£ã‚ª","ãƒãƒãƒƒãƒˆ","ãƒ†ã‚£ãƒ¼ãƒ€","ã‚µã‚¯ãƒ©"
]

GEN_PREFIXES = [("æ–°å‹","æ–°å‹"), ("ç¾è¡Œ","ç¾è¡Œ"), ("æ—§å‹","æ—§å‹"), ("å…ˆä»£","æ—§å‹")]

def detect_brand_name(title: str) -> str|None:
    for pat, name in BRAND_PATTERNS:
        if pat.search(title):
            return name
    return None

def detect_nissan_model(title: str) -> str|None:
    for m in NISSAN_MODELS:
        if m.lower() in title.lower():
            return m
    return None

def build_car_category(title: str) -> str:
    """è»Šã‚«ãƒ†ã‚´ãƒªã®æœ€çµ‚è¡¨è¨˜ã‚’è¿”ã™ï¼ˆãƒ‹ãƒƒã‚µãƒ³è»Šã¯è»Šï¼ˆæ–°å‹XXXï¼‰ç­‰ã€ä»–ç¤¾ã¯è»Šï¼ˆç«¶åˆï¼‰ï¼‰ã€‚"""
    model = detect_nissan_model(title)
    if model:
        # æ–°å‹/ç¾è¡Œ/æ—§å‹ å‰ç½®ãæ¤œå‡º
        prefix = ""
        for key, norm in GEN_PREFIXES:
            if key in title:
                prefix = norm
                break
        label = f"{prefix}{model}" if prefix else model
        return f"è»Šï¼ˆ{label}ï¼‰"
    # ãƒ‹ãƒƒã‚µãƒ³ä»¥å¤–ã®è»Šåï¼ˆç°¡æ˜“åˆ¤å®šï¼‰ï¼šä»–ç¤¾è»Šåãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ãã† & ã€Œè»Š/EV/ãƒ¢ãƒ‡ãƒ«/â—‹â—‹ã€ãªã©
    # å³å¯†ã«ã¯è¾æ›¸ãŒå¿…è¦ã ãŒã€è¦ä»¶ã§ã¯ãƒ‹ãƒƒã‚µãƒ³ä»¥å¤–ã¯ã€Œè»Šï¼ˆç«¶åˆï¼‰ã€å›ºå®šã§OK
    # è»Šåã‚‰ã—ã„å˜èªãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹
    carish = any(k in title for k in ["æ–°å‹","ãƒ¢ãƒ‡ãƒ«","ã‚°ãƒ¬ãƒ¼ãƒ‰","ç™ºè¡¨","ç™ºå£²","SUV","ã‚»ãƒ€ãƒ³","ãƒãƒƒãƒãƒãƒƒã‚¯","ã‚¯ãƒ¼ãƒš","ãƒŸãƒ‹ãƒãƒ³"])
    other_brand = detect_brand_name(title)
    if other_brand and other_brand != "ãƒ‹ãƒƒã‚µãƒ³" and carish:
        return "è»Šï¼ˆç«¶åˆï¼‰"
    # è»Šã‚«ãƒ†ã‚´ãƒªã¨æ–­å®šã§ããªã‘ã‚Œã° Noneï¼ˆå‘¼ã³å‡ºã—å´ã§åˆ¥ã‚«ãƒ†ã‚´ãƒªã«ã™ã‚‹ï¼‰
    return None

def build_company_category(title: str) -> str:
    brand = detect_brand_name(title)
    return f"ä¼šç¤¾ï¼ˆ{brand if brand else 'ãã®ä»–'}ï¼‰"

def normalize_category(title: str, gemini_cat: str) -> str:
    """Geminiã®ã–ã£ãã‚Šã‚«ãƒ†ã‚´ãƒªå€™è£œã‚’å—ã‘ã€æœ€çµ‚ã‚«ãƒ†ã‚´ãƒªæ–‡å­—åˆ—ã«æ­£è¦åŒ–"""
    base = (gemini_cat or "").strip()
    # æŠ€è¡“ç³»ã¯ãã®ã¾ã¾å„ªå…ˆ
    tech_whitelist = [
        "æŠ€è¡“ï¼ˆEVï¼‰","æŠ€è¡“ï¼ˆe-POWERï¼‰","æŠ€è¡“ï¼ˆe-4ORCEï¼‰","æŠ€è¡“ï¼ˆAD/ADASï¼‰","æŠ€è¡“",
        "ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚¹ãƒãƒ¼ãƒ„","æ ªå¼","æ”¿æ²»ãƒ»çµŒæ¸ˆ","ã‚¹ãƒãƒ¼ãƒ„","ãã®ä»–"
    ]
    if base in tech_whitelist:
        return base

    # è»Šã®å³æ ¼è¡¨è¨˜
    if base.startswith("è»Š"):
        car_cat = build_car_category(title)
        if car_cat:
            return car_cat
        # GeminiãŒè»Šã¨è¨€ã£ãŸãŒæ ¹æ‹ å¼±ã„å ´åˆã¯ä¼šç¤¾æ‰±ã„ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return build_company_category(title)

    # ä¼šç¤¾ã¯å¿…ãšï¼ˆãƒ–ãƒ©ãƒ³ãƒ‰ï¼‰ã‚’ä»˜ä¸
    if base == "ä¼šç¤¾" or base == "ä¼æ¥­":
        return build_company_category(title)

    # è»Šï¼ˆç«¶åˆï¼‰ã¨æ˜ç¤ºã•ã‚ŒãŸå ´åˆã¯ãã®ã¾ã¾
    if base == "è»Šï¼ˆç«¶åˆï¼‰":
        return "è»Šï¼ˆç«¶åˆï¼‰"

    # ãã®ä»–/ä¸æ˜ã¯ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹
    # æ˜ã‚‰ã‹ã«ä¼šç¤¾ã£ã½ã„ï¼ˆç”Ÿç”£/è²©å£²/æ¥­ç¸¾/ãƒªã‚³ãƒ¼ãƒ« ç­‰ï¼‰
    if any(k in title for k in ["è²©å£²", "ç”Ÿç”£", "ãƒªã‚³ãƒ¼ãƒ«", "ææº", "çµ±åˆ", "å‡ºè·", "å‡ºè³‡", "æŠ•è³‡", "å·¥å ´", "é›‡ç”¨", "å†å»º", "æ’¤é€€", "å­ä¼šç¤¾", "äººå“¡"]):
        return build_company_category(title)

    # è»Šåã‚’å¼·ãå«ã‚€ãªã‚‰è»Šå´ã«å¯„ã›ã‚‹
    car_cat = build_car_category(title)
    if car_cat:
        return car_cat

    # æœ€å¾Œã¯ãã®ä»–
    return "ãã®ä»–"

# ================== Gemini å®‰å®šåŒ–ï¼ˆJSONå¼·åˆ¶ï¼‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ ==================
def _extract_json_array(text: str):
    if not text:
        return None
    s = text.find("[")
    e = text.rfind("]")
    if s == -1 or e == -1 or e <= s:
        return None
    try:
        return json.loads(text[s:e+1])
    except Exception:
        return None

def _heuristic_sentiment(title: str) -> str:
    neg_kw = ["åœæ­¢", "çµ‚äº†", "æ’¤é€€", "ä¸ç¥¥äº‹", "ä¸‹è½", "å¦å®š", "ç‚ä¸Š", "äº‹æ•…", "å•é¡Œ", "ç ´è«‡", "äººå“¡å‰Šæ¸›", "é›‡ç”¨ä¸å®‰"]
    pos_kw = ["ç™ºè¡¨", "å—è³", "å¥½èª¿", "ä¸Šæ˜‡", "ç™»å ´", "å…¬é–‹", "æ–°å‹", "å¼·åŒ–", "å—æ³¨", "ç™ºå£²", "ãƒ©ã‚¤ãƒ³ãƒŠãƒƒãƒ—", "å¢—åŠ "]
    if any(k in title for k in neg_kw):
        return "ãƒã‚¬ãƒ†ã‚£ãƒ–"
    if any(k in title for k in pos_kw):
        return "ãƒã‚¸ãƒ†ã‚£ãƒ–"
    return "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«"

def classify_titles_gemini_batched(titles: list[str], batch_size: int = 80) -> list[tuple[str, str]]:
    """æˆ»ã‚Šå€¤: [(sentiment, normalized_category), ...]"""
    if not titles:
        return []
    if not GEMINI_API_KEY:
        # Geminiãªã—â†’æ„Ÿæƒ…ã¯ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã€ã‚«ãƒ†ã‚´ãƒªã¯è‡ªåŠ›æ­£è¦åŒ–ï¼ˆå…ˆã«ä¼šç¤¾/è»Šã®å½“ã¦ã¯ã‚ï¼‰
        out = []
        for t in titles:
            s = _heuristic_sentiment(t)
            # ç²—ã‚«ãƒ†ã‚´ãƒªå€™è£œã‚’è‡ªå‰æ¨å®š
            rough = "ä¼šç¤¾" if ("æ—¥ç”£" in t or "ãƒ‹ãƒƒã‚µãƒ³" in t) else "ãã®ä»–"
            norm = normalize_category(t, rough)
            out.append((s, norm))
        return out

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(
        "gemini-1.5-flash",
        generation_config={"response_mime_type": "application/json"}
    )

    out = [("", "")] * len(titles)
    for start in range(0, len(titles), batch_size):
        batch = titles[start:start+batch_size]
        payload = [{"row": start+i, "title": t} for i, t in enumerate(batch)]
        sys_prompt = (
            "ã‚ãªãŸã¯æ•è…•é›‘èªŒè¨˜è€…ã§ã™ã€‚å„ã‚¿ã‚¤ãƒˆãƒ«ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’åˆ¤å®šã—ã€"
            "JSONé…åˆ—ã®ã¿ã§è¿”ã—ã¦ãã ã•ã„ã€‚å„è¦ç´ ã¯ "
            '{"row": æ•°å€¤, "sentiment": "ãƒã‚¸ãƒ†ã‚£ãƒ–|ãƒã‚¬ãƒ†ã‚£ãƒ–|ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", '
            '"category": "ä¼šç¤¾|è»Š|è»Šï¼ˆç«¶åˆï¼‰|æŠ€è¡“ï¼ˆEVï¼‰|æŠ€è¡“ï¼ˆe-POWERï¼‰|æŠ€è¡“ï¼ˆe-4ORCEï¼‰|'
            'æŠ€è¡“ï¼ˆAD/ADASï¼‰|æŠ€è¡“|ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚¹ãƒãƒ¼ãƒ„|æ ªå¼|æ”¿æ²»ãƒ»çµŒæ¸ˆ|ã‚¹ãƒãƒ¼ãƒ„|ãã®ä»–"}ã€‚'
            "ã‚¿ã‚¤ãƒˆãƒ«ã¯æ”¹å¤‰ã—ãªã„ã€‚ã‚«ãƒ†ã‚´ãƒªã¯å˜ä¸€ã€‚"
        )
        try:
            resp = model.generate_content([
                sys_prompt,
                {"mime_type": "application/json", "text": json.dumps(payload, ensure_ascii=False)}
            ])
            text = (getattr(resp, "text", "") or "").strip()
            arr = None
            try:
                if text:
                    arr = json.loads(text)
            except Exception:
                arr = _extract_json_array(text)
            if isinstance(arr, dict):
                arr = [arr]
            # åæ˜ 
            if isinstance(arr, list):
                for obj in arr:
                    try:
                        idx = int(obj.get("row"))
                        s = str(obj.get("sentiment", "")).strip()
                        c = str(obj.get("category", "")).strip()
                        if 0 <= idx < len(out):
                            out[idx] = (s, c)
                    except Exception:
                        continue
            # æ­£è¦åŒ–ï¼‹åŸ‹ã‚
            for i in range(start, start+len(batch)):
                s, c = out[i]
                s = s or _heuristic_sentiment(titles[i])
                c = normalize_category(titles[i], c or "ãã®ä»–")
                out[i] = (s, c)
        except Exception as e:
            print(f"Geminiãƒãƒƒãƒå¤±æ•—: {e}")
            for i in range(start, start+len(batch)):
                s = _heuristic_sentiment(titles[i])
                c = normalize_category(titles[i], "ãã®ä»–")
                out[i] = (s, c)
        time.sleep(0.2)
    return out

# ================== é›†ç´„ï¼ˆæ˜¨æ—¥15:00ã€œä»Šæ—¥14:59ã€ã‚·ãƒ¼ãƒˆå=ä»Šæ—¥ã®YYMMDDï¼‰ ==================
def build_daily_sheet(sh, msn_items, google_items, yahoo_items):
    now = now_jst()
    today_1500 = now.replace(hour=15, minute=0, second=0, microsecond=0)
    start = today_1500 - timedelta(days=1)  # æ˜¨æ—¥15:00
    end = today_1500                        # ä»Šæ—¥14:59:59 ã¾ã§ï¼ˆ< endï¼‰
    sheet_name = now.strftime("%y%m%d")

    def in_window(pub_str: str) -> bool:
        try:
            dt = datetime.strptime(pub_str, "%Y/%m/%d %H:%M").replace(tzinfo=JST)
            return start <= dt < end
        except Exception:
            return False

    msn_f    = [x for x in msn_items if in_window(x[3])]
    google_f = [x for x in google_items if in_window(x[3])]
    yahoo_f  = [x for x in yahoo_items if in_window(x[3])]

    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿çµæœ: MSN={len(msn_f)}, Google={len(google_f)}, Yahoo={len(yahoo_f)}")

    # ä¸¦ã³ï¼šMSNâ†’Googleâ†’Yahoo
    ordered = msn_f + google_f + yahoo_f

    # ã‚¿ã‚¤ãƒˆãƒ«ä¸€æ‹¬åˆ†é¡ï¼ˆGeminiâ†’å³æ ¼æ­£è¦åŒ–ï¼‰
    titles = [row[2] for row in ordered]
    senti_cate = classify_titles_gemini_batched(titles)

    # ã‚·ãƒ¼ãƒˆä½œæˆï¼ˆæ—¢å­˜ãªã‚‰ã‚¯ãƒªã‚¢ï¼‰
    try:
        ws = sh.worksheet(sheet_name)
        ws.clear()
    except Exception:
        ws = sh.add_worksheet(title=sheet_name, rows="5000", cols="10")

    headers = ["ã‚½ãƒ¼ã‚¹", "URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª"]
    ws.update([headers], "A1:H1")  # values first, then range

    rows = []
    for i, row in enumerate(ordered):
        src, url, title, pub, origin = row[:5]
        comment = ""  # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¯ç¾åœ¨ã‚¹ã‚­ãƒƒãƒ—
        s, c = senti_cate[i] if i < len(senti_cate) else ("", "")
        rows.append([src, url, title, pub, origin, comment, s, c])

    if rows:
        ws.update(rows, f"A2:H{len(rows)+1}")  # values first, then range

    print(f"ğŸ•’ é›†ç´„æœŸé–“: {start.strftime('%Y/%m/%d %H:%M')} ã€œ {(end - timedelta(minutes=1)).strftime('%Y/%m/%d %H:%M')} â†’ ã‚·ãƒ¼ãƒˆå: {sheet_name}")
    print(f"âœ… é›†ç´„ã‚·ãƒ¼ãƒˆ {sheet_name}: {len(rows)} ä»¶")
    return sheet_name

# ================== Main ==================
def main():
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {NEWS_KEYWORD}")
    print(f"ğŸ“„ SPREADSHEET_ID: {SPREADSHEET_ID}")
    if not SPREADSHEET_ID:
        raise RuntimeError("SPREADSHEET_ID ãŒæœªè¨­å®šã§ã™ã€‚")

    gc = get_gspread_client()
    sh = gc.open_by_key(SPREADSHEET_ID)
    print(f"ğŸ“˜ Opened spreadsheet title: {sh.title}")

    print("\n--- å–å¾— ---")
    google_items = fetch_google_news(NEWS_KEYWORD)
    yahoo_items  = fetch_yahoo_news(NEWS_KEYWORD)
    msn_items    = fetch_msn_news(NEWS_KEYWORD)

    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(google_items)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾— {sum(1 for i in google_items if i[3])} ä»¶ï¼‰")
    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(yahoo_items)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾— {sum(1 for i in yahoo_items if i[3])} ä»¶ï¼‰")
    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(msn_items)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾—/æ¨å®š {sum(1 for i in msn_items if i[3])} ä»¶ï¼‰")

    print("\n--- é›†ç´„ï¼ˆã¾ã¨ã‚ã‚·ãƒ¼ãƒˆã®ã¿ / Aåˆ—=ã‚½ãƒ¼ã‚¹ / é †=MSNâ†’Googleâ†’Yahooï¼‰ ---")
    build_daily_sheet(sh, msn_items, google_items, yahoo_items)

if __name__ == "__main__":
    main()
