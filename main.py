# -*- coding: utf-8 -*-
"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹é›†ç´„ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã¾ã¨ã‚ã‚·ãƒ¼ãƒˆã®ã¿å‡ºåŠ›ï¼‰
- MSN / Google / Yahoo ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ï¼ˆSeleniumï¼‰
- Yahooã¯ãƒã‚¤ã‚ºé™¤å¤–ï¼ˆ/articles/ ã¾ãŸã¯ /pickup/ ã®ã¿ï¼‰ã—ã€è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ•ç¨¿æ—¥ã‚‚è£œå®Œ
- æ—¥ä»˜ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆå‰æ—¥15:00ã€œå½“æ—¥14:59 JSTï¼‰ã§ã€ŒYYMMDDã€ã‚·ãƒ¼ãƒˆã«é›†ç´„
- é›†ç´„ã‚·ãƒ¼ãƒˆã¯ Aåˆ—=ã‚½ãƒ¼ã‚¹åã€ä¸¦ã³ã¯ MSN â†’ Google â†’ Yahooï¼ˆå„ã‚½ãƒ¼ã‚¹å†…ã¯æŠ•ç¨¿æ—¥é™é †ï¼‰

ç’°å¢ƒå¤‰æ•°:
- GCP_SERVICE_ACCOUNT_KEY: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONæ–‡å­—åˆ—ï¼ˆGitHub Secrets æ¨å¥¨ï¼‰
  ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã¯ credentials.json ã§ã‚‚å¯ï¼‰
- NEWS_KEYWORD: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©: "æ—¥ç”£"ï¼‰
- SPREADSHEET_ID: å‡ºåŠ›å…ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDï¼ˆãƒ‡ãƒ•ã‚©ã¯ã”æŒ‡å®šIDï¼‰
"""

import os
import re
import json
import time
import requests
from datetime import datetime, timedelta, time as dtime
from email.utils import parsedate_to_datetime

import gspread
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager


# ========= è¨­å®š =========
KEYWORD = os.getenv("NEWS_KEYWORD", "æ—¥ç”£")
SPREADSHEET_ID = os.getenv(
    "SPREADSHEET_ID",
    "1Vs4Cx8QPN4H2NOgtwaviOCe8zBTpUNDgJjqkHr51IZE"
)


# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def jst_now() -> datetime:
    return datetime.utcnow() + timedelta(hours=9)


def fmt(dt: datetime) -> str:
    return dt.strftime("%Y/%m/%d %H:%M")


def try_parse_jst(dt_str: str):
    """ä»£è¡¨çš„ãªæ—¥æ™‚æ–‡å­—åˆ—â†’datetimeï¼ˆJSTï¼‰ã€‚å¤±æ•—æ™‚ã¯ None"""
    if not dt_str or dt_str == "å–å¾—ä¸å¯":
        return None
    patterns = [
        "%Y/%m/%d %H:%M",
        "%Y/%m/%d %H:%M:%S",
        "%Y/%m/%d",
        "%Y-%m-%d %H:%M",
        "%Y-%m-%dT%H:%M:%SZ",  # UTC â†’ JST
        "%Y-%m-%dT%H:%M:%S%z", # tz aware
    ]
    for p in patterns:
        try:
            dt = datetime.strptime(dt_str, p)
            if p.endswith("Z"):
                dt = dt + timedelta(hours=9)
            elif "%z" in p:
                # tz-aware â†’ JSTã¸
                dt = dt.astimezone(tz=None)  # UTCãƒ­ãƒ¼ã‚«ãƒ«
                dt = dt + timedelta(hours=9)
            return dt
        except Exception:
            pass
    return None


def parse_relative_time(label: str, base: datetime) -> str:
    """
    ã€Œâ—¯åˆ†å‰ / â—¯æ™‚é–“å‰ / â—¯æ—¥å‰ã€ã€ŒMMæœˆDDæ—¥ã€ã€ŒHH:MMã€ãªã©ã‚’ JST çµ¶å¯¾æ™‚åˆ»ã«ã€‚
    å¤±æ•—æ™‚ã¯ "å–å¾—ä¸å¯"
    """
    s = (label or "").strip()
    try:
        m = re.search(r"(\d+)\s*åˆ†å‰", s)
        if m:
            return fmt(base - timedelta(minutes=int(m.group(1))))
        h = re.search(r"(\d+)\s*æ™‚é–“å‰", s)
        if h:
            return fmt(base - timedelta(hours=int(h.group(1))))
        d = re.search(r"(\d+)\s*æ—¥å‰", s)
        if d:
            return fmt(base - timedelta(days=int(d.group(1))))
        if re.match(r"\d{1,2}æœˆ\d{1,2}æ—¥$", s):
            dt = datetime.strptime(f"{base.year}å¹´{s}", "%Yå¹´%mæœˆ%dæ—¥")
            return fmt(dt)
        if re.match(r"\d{4}/\d{1,2}/\d{1,2}$", s):
            dt = datetime.strptime(s, "%Y/%m/%d")
            return fmt(dt)
        if re.match(r"\d{1,2}:\d{2}$", s):
            t = datetime.strptime(s, "%H:%M").time()
            dt = datetime.combine(base.date(), t)
            if dt > base:
                dt -= timedelta(days=1)
            return fmt(dt)
    except Exception:
        pass
    return "å–å¾—ä¸å¯"


def get_last_modified_datetime(url: str) -> str:
    """HTTPãƒ˜ãƒƒãƒ€ Last-Modified â†’ JSTï¼ˆæœ€çµ‚æ‰‹æ®µï¼‰"""
    try:
        r = requests.head(url, timeout=6, allow_redirects=True)
        if "Last-Modified" in r.headers:
            dt = parsedate_to_datetime(r.headers["Last-Modified"])
            if dt.tzinfo:
                dt = dt.astimezone(tz=None)  # UTCãƒ­ãƒ¼ã‚«ãƒ«
                dt = dt + timedelta(hours=9)
            else:
                dt = dt + timedelta(hours=9)
            return fmt(dt)
    except Exception:
        pass
    return "å–å¾—ä¸å¯"


def fetch_html(url: str, timeout: int = 10):
    try:
        hdrs = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=hdrs, timeout=timeout)
        if r.ok:
            return r.text
    except Exception:
        pass
    return ""


def extract_yahoo_article_datetime(html: str) -> str:
    """Yahooè¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ•ç¨¿æ—¥ã‚’æ¨å®šï¼ˆè¤‡æ•°å€™è£œã‚’é †ã«ãƒã‚§ãƒƒã‚¯ï¼‰"""
    if not html:
        return "å–å¾—ä¸å¯"
    soup = BeautifulSoup(html, "html.parser")

    # 1) <time datetime="...">
    t = soup.find("time", attrs={"datetime": True})
    if t and t.get("datetime"):
        cand = t["datetime"].strip()
        # ä¾‹: 2025-08-20T09:00:00+09:00 / 2025-08-20T00:00:00Z
        dt = try_parse_jst(cand)
        if dt:
            return fmt(dt)

    # 2) meta[property=article:published_time] / article:modified_time / og:updated_time
    for prop in ["article:published_time", "article:modified_time", "og:updated_time"]:
        m = soup.find("meta", attrs={"property": prop, "content": True})
        if m and m.get("content"):
            dt = try_parse_jst(m["content"].strip())
            if dt:
                return fmt(dt)

    return "å–å¾—ä¸å¯"


def chrome_driver():
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


# ========= ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ =========
def get_google_news(keyword: str):
    driver = chrome_driver()
    url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    driver.get(url)
    time.sleep(4)
    for _ in range(3):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.4)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data = []
    for art in soup.find_all("article"):
        try:
            a = art.select_one("a.JtKRv, a.WwrzSb")
            t = art.select_one("time[datetime]")
            src = art.select_one("div.vr1PYe, div.SVJrMe")
            if not a or not t:
                continue
            title = a.get_text(strip=True)
            href = a.get("href")
            url = "https://news.google.com" + href[1:] if href and href.startswith("./") else href
            dt = datetime.strptime(t.get("datetime"), "%Y-%m-%dT%H:%M:%SZ") + timedelta(hours=9)
            pub = fmt(dt)
            source = src.get_text(strip=True) if src else "Google"
            if title and url and url.startswith("http"):
                data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": pub, "å¼•ç”¨å…ƒ": source, "ã‚½ãƒ¼ã‚¹": "Google"})
        except Exception:
            continue
    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(data)} ä»¶")
    return data


def get_yahoo_news(keyword: str):
    driver = chrome_driver()
    url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(url)
    time.sleep(4)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data = []
    # ãƒã‚¤ã‚ºé™¤å¤–ã®ãŸã‚ã€è¨˜äº‹URLã®ã¿æ¡ç”¨
    def is_article(u: str) -> bool:
        if not u or not u.startswith("http"):
            return False
        return ("news.yahoo.co.jp/articles/" in u) or ("news.yahoo.co.jp/pickup/" in u)

    # åºƒã‚ã« a[href] ã‚’æ‹¾ã„ã€è¨˜äº‹URLã®ã¿æ®‹ã™
    for a in soup.find_all("a", href=True):
        try:
            href = a["href"]
            if not is_article(href):
                continue
            title = a.get_text(strip=True)
            if not title or len(title) < 6:
                # ã‚µãƒ ãƒ/ã‚«ãƒ†ã‚´ãƒªç­‰ã®çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å¼¾ã
                continue

            # å¯èƒ½ãªã‚‰è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰æ—¥æ™‚è£œå®Œ
            html = fetch_html(href)
            pub = extract_yahoo_article_datetime(html)
            # å‡ºå…¸ï¼ˆmediaåï¼‰ã‚‚æ‹¾ãˆãŸã‚‰è¼‰ã›ã‚‹ï¼ˆç„¡ã‘ã‚Œã° "Yahoo"ï¼‰
            source = "Yahoo"
            if html:
                soup2 = BeautifulSoup(html, "html.parser")
                m = soup2.find("meta", attrs={"name": "source", "content": True})
                if m and m.get("content"):
                    source = m["content"].strip() or "Yahoo"
                # ä»£æ›¿ã§ã€è¨˜äº‹ãƒ˜ãƒƒãƒ€ä»˜è¿‘ã®åª’ä½“åã£ã½ã„è¦ç´ ã‚’æ‹¾ã†ç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯
                if source == "Yahoo":
                    cand = soup2.find(["span","div"], string=re.compile(r".+"))
                    if cand:
                        txt = cand.get_text(strip=True)
                        if 2 <= len(txt) <= 20 and not txt.isdigit():
                            source = txt

            data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": href, "æŠ•ç¨¿æ—¥": pub, "å¼•ç”¨å…ƒ": source, "ã‚½ãƒ¼ã‚¹": "Yahoo"})
        except Exception:
            continue

    # é‡è¤‡URLé™¤å»
    uniq = []
    seen = set()
    for d in data:
        if d["URL"] not in seen:
            seen.add(d["URL"])
            uniq.append(d)

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆè¨˜äº‹ã®ã¿ï¼‰: {len(uniq)} ä»¶")
    return uniq


def get_msn_news(keyword: str):
    base = jst_now()
    driver = chrome_driver()
    url = f"https://www.bing.com/news/search?q={keyword}&qft=sortbydate%3d'1'&form=YFNR"
    driver.get(url)
    time.sleep(4)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data = []
    cards = soup.select("div.news-card, news-card")
    for c in cards:
        try:
            title = (c.get("data-title") or "").strip()
            link = (c.get("data-url") or "").strip()
            author = (c.get("data-author") or "").strip()
            if not title or not link or not link.startswith("http"):
                continue

            pub_label = ""
            span = c.find("span", attrs={"aria-label": True})
            if span and span.has_attr("aria-label"):
                pub_label = span["aria-label"].strip()

            pub = parse_relative_time(pub_label, base)
            if pub == "å–å¾—ä¸å¯":
                pub = get_last_modified_datetime(link)

            data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": link, "æŠ•ç¨¿æ—¥": pub, "å¼•ç”¨å…ƒ": author or "MSN", "ã‚½ãƒ¼ã‚¹": "MSN"})
        except Exception:
            continue

    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(data)} ä»¶")
    return data


# ========= ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ =========
def get_gspread_client():
    key = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
    if key:
        try:
            creds = json.loads(key)
            return gspread.service_account_from_dict(creds)
        except Exception as e:
            raise RuntimeError(f"GCP_SERVICE_ACCOUNT_KEY ã®JSONãŒä¸æ­£ã§ã™: {e}")
    return gspread.service_account(filename="credentials.json")


def compute_window(now_jst: datetime):
    """
    å®Ÿè¡ŒãŒ 15:00 ä»¥é™:   æ˜¨æ—¥15:00ã€œä»Šæ—¥14:59:59 / ã‚·ãƒ¼ãƒˆå=ä»Šæ—¥ã®YYMMDD
    å®Ÿè¡ŒãŒ 15:00 ã‚ˆã‚Šå‰: ä¸€æ˜¨æ—¥15:00ã€œæ˜¨æ—¥14:59:59 / ã‚·ãƒ¼ãƒˆå=æ˜¨æ—¥ã®YYMMDD
    """
    today = now_jst.date()
    fifteen = datetime.combine(today, dtime(hour=15, minute=0))
    if now_jst >= fifteen:
        start = fifteen - timedelta(days=1)
        end = fifteen - timedelta(seconds=1)
        label = today.strftime("%y%m%d")
    else:
        start = fifteen - timedelta(days=2)
        end = fifteen - timedelta(days=1, seconds=1)
        label = (today - timedelta(days=1)).strftime("%y%m%d")
    return start, end, label


def build_daily_sheet(sh, rows_all: list):
    """
    rows_all: [ {"ã‚½ãƒ¼ã‚¹","URL","ã‚¿ã‚¤ãƒˆãƒ«","æŠ•ç¨¿æ—¥","å¼•ç”¨å…ƒ"} ... ]
    ä¸¦ã³: MSN â†’ Google â†’ Yahooï¼ˆå„ã‚½ãƒ¼ã‚¹å†…ã¯æŠ•ç¨¿æ—¥é™é †ï¼‰
    """
    now = jst_now()
    start, end, label = compute_window(now)
    print(f"ğŸ•’ é›†ç´„æœŸé–“: {fmt(start)} ã€œ {fmt(end)} â†’ ã‚·ãƒ¼ãƒˆå: {label}")

    # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ã¿ã«çµã‚‹
    filtered_by_src = {"MSN": [], "Google": [], "Yahoo": []}
    for r in rows_all:
        dt = try_parse_jst(r.get("æŠ•ç¨¿æ—¥", ""))
        if dt and (start <= dt <= end):
            src = r.get("ã‚½ãƒ¼ã‚¹", "")
            if src in filtered_by_src:
                filtered_by_src[src].append(r)

    # ã‚½ãƒ¼ã‚¹å†…URLå»é‡ & æŠ•ç¨¿æ—¥é™é †
    def dedup_sort(lst):
        seen = set()
        uniq = []
        for d in lst:
            if d["URL"] not in seen:
                seen.add(d["URL"])
                uniq.append(d)
        uniq.sort(key=lambda x: try_parse_jst(x["æŠ•ç¨¿æ—¥"]) or datetime(1970,1,1), reverse=True)
        return uniq

    ordered = []
    for src in ["MSN", "Google", "Yahoo"]:
        ordered.extend(dedup_sort(filtered_by_src[src]))

    # å‡ºåŠ›
    headers = ["ã‚½ãƒ¼ã‚¹", "URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"]
    try:
        ws = sh.worksheet(label)
        ws.clear()
        ws.append_row(headers)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=label, rows=str(max(2, len(ordered)+5)), cols="5")
        ws.append_row(headers)

    if ordered:
        rows = [[d["ã‚½ãƒ¼ã‚¹"], d["URL"], d["ã‚¿ã‚¤ãƒˆãƒ«"], d["æŠ•ç¨¿æ—¥"], d["å¼•ç”¨å…ƒ"]] for d in ordered]
        ws.append_rows(rows, value_input_option="USER_ENTERED")
        print(f"âœ… é›†ç´„ã‚·ãƒ¼ãƒˆ {label}: {len(rows)} ä»¶")
    else:
        print(f"âš ï¸ é›†ç´„ã‚·ãƒ¼ãƒˆ {label}: å¯¾è±¡è¨˜äº‹ãªã—")


# ========= ãƒ¡ã‚¤ãƒ³ =========
def main():
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {KEYWORD}")
    print(f"ğŸ“„ SPREADSHEET_ID(envå„ªå…ˆ): {SPREADSHEET_ID}")

    gc = get_gspread_client()
    sh = gc.open_by_key(SPREADSHEET_ID)
    print(f"ğŸ“˜ Opened spreadsheet title: {sh.title}")

    print("\n--- å–å¾— ---")
    google_items = get_google_news(KEYWORD)
    yahoo_items  = get_yahoo_news(KEYWORD)
    msn_items    = get_msn_news(KEYWORD)

    # ã¾ã¨ã‚ã‚·ãƒ¼ãƒˆç”¨ã«çµåˆï¼ˆå€‹åˆ¥ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã¯ã—ãªã„ï¼‰
    all_items = []
    all_items.extend(msn_items)
    all_items.extend(google_items)
    all_items.extend(yahoo_items)

    print("\n--- é›†ç´„ï¼ˆã¾ã¨ã‚ã‚·ãƒ¼ãƒˆã®ã¿å‡ºåŠ›: MSNâ†’Googleâ†’Yahoo / Aåˆ—=ã‚½ãƒ¼ã‚¹ï¼‰ ---")
    build_daily_sheet(sh, all_items)


if __name__ == "__main__":
    main()
