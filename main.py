# -*- coding: utf-8 -*-
"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹é›†ç´„ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- MSN / Google / Yahoo ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ï¼ˆSeleniumï¼‰
- å–å¾—çµæœã‚’å„ã‚½ãƒ¼ã‚¹å°‚ç”¨ã‚·ãƒ¼ãƒˆï¼ˆGoogle / Yahoo / MSNï¼‰ã¸URLå»é‡ã§è¿½è¨˜
- æ—¥ä»˜ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆå‰æ—¥15:00ã€œå½“æ—¥14:59 JSTï¼‰ã§ã€ŒYYMMDDã€ã‚·ãƒ¼ãƒˆã«é›†ç´„
- é›†ç´„ã‚·ãƒ¼ãƒˆã¯ Aåˆ—ã«ã‚½ãƒ¼ã‚¹åã€ä¸¦ã³é †ã¯ MSN â†’ Google â†’ Yahooï¼ˆå„ã‚½ãƒ¼ã‚¹å†…ã¯æŠ•ç¨¿æ—¥é™é †ï¼‰

ç’°å¢ƒå¤‰æ•°:
- GCP_SERVICE_ACCOUNT_KEY: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆJSONã®æ–‡å­—åˆ—ï¼ˆGitHub Secrets æ¨å¥¨ï¼‰
  * ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã¯åŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« credentials.json ã§ã‚‚å¯
- NEWS_KEYWORD: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: "æ—¥ç”£"ï¼‰
- SPREADSHEET_ID: æ›¸ãè¾¼ã¿å…ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID
  * ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1Vs4Cx8QPN4H2NOgtwaviOCe8zBTpUNDgJjqkHr51IZE
"""

import os
import re
import json
import time
import requests
from datetime import datetime, timedelta, time as dtime
from email.utils import parsedate_to_datetime

import gspread
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager


# ========= è¨­å®š =========
KEYWORD = os.getenv("NEWS_KEYWORD", "æ—¥ç”£")
SPREADSHEET_ID = os.getenv(
    "SPREADSHEET_ID",
    "1Vs4Cx8QPN4H2NOgtwaviOCe8zBTpUNDgJjqkHr51IZE"  # ã”æŒ‡å®šã®IDã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
)


# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def jst_now() -> datetime:
    return datetime.utcnow() + timedelta(hours=9)


def fmt(dt: datetime) -> str:
    return dt.strftime("%Y/%m/%d %H:%M")


def try_parse_jst(dt_str: str):
    """ 'YYYY/MM/DD HH:MM' ãªã©ã‚’ datetime(JST) ã«ã€‚å¤±æ•—ã¯ None """
    if not dt_str or dt_str == "å–å¾—ä¸å¯":
        return None
    patterns = [
        "%Y/%m/%d %H:%M",
        "%Y/%m/%d %H:%M:%S",
        "%Y/%m/%d",
        "%Y-%m-%d %H:%M",
        "%Y-%m-%dT%H:%M:%SZ",  # Zã¯UTCæƒ³å®šâ†’JSTã¸
    ]
    for p in patterns:
        try:
            dt = datetime.strptime(dt_str, p)
            if p.endswith("Z"):
                dt = dt + timedelta(hours=9)
            return dt
        except Exception:
            pass
    return None


def parse_relative_time(label: str, base: datetime) -> str:
    """
    ã€Œâ—¯åˆ†å‰ / â—¯æ™‚é–“å‰ / â—¯æ—¥å‰ã€ã€ŒMMæœˆDDæ—¥ã€ã€ŒHH:MMã€ç­‰ã‚’ JST çµ¶å¯¾æ™‚åˆ»ã¸
    å¤±æ•—æ™‚ã¯ "å–å¾—ä¸å¯"
    """
    s = (label or "").strip()
    try:
        # â—¯åˆ†å‰
        m = re.search(r"(\d+)\s*åˆ†å‰", s)
        if m:
            return fmt(base - timedelta(minutes=int(m.group(1))))
        # â—¯æ™‚é–“å‰
        h = re.search(r"(\d+)\s*æ™‚é–“å‰", s)
        if h:
            return fmt(base - timedelta(hours=int(h.group(1))))
        # â—¯æ—¥å‰
        d = re.search(r"(\d+)\s*æ—¥å‰", s)
        if d:
            return fmt(base - timedelta(days=int(d.group(1))))
        # ä¾‹) 8æœˆ20æ—¥ / 08æœˆ20æ—¥
        if re.match(r"\d{1,2}æœˆ\d{1,2}æ—¥", s):
            dt = datetime.strptime(f"{base.year}å¹´{s}", "%Yå¹´%mæœˆ%dæ—¥")
            return fmt(dt)
        # ä¾‹) 2025/08/20
        if re.match(r"\d{4}/\d{1,2}/\d{1,2}$", s):
            dt = datetime.strptime(s, "%Y/%m/%d")
            return fmt(dt)
        # ä¾‹) 12:34ï¼ˆå½“æ—¥ã‹å‰æ—¥ï¼‰
        if re.match(r"\d{1,2}:\d{2}$", s):
            t = datetime.strptime(s, "%H:%M").time()
            dt = datetime.combine(base.date(), t)
            if dt > base:
                dt -= timedelta(days=1)
            return fmt(dt)
    except Exception:
        pass
    return "å–å¾—ä¸å¯"


def get_last_modified_datetime(url: str) -> str:
    """ HEADã®Last-Modifiedã‹ã‚‰JSTã‚’æ¨å®šï¼ˆãªã‘ã‚Œã°å–å¾—ä¸å¯ï¼‰ """
    try:
        r = requests.head(url, timeout=5, allow_redirects=True)
        if "Last-Modified" in r.headers:
            dt = parsedate_to_datetime(r.headers["Last-Modified"])
            # tz-aware ã®å ´åˆã¯UTCåŸºæº–ã€naiveã¯ä¸€å¿œUTCã¨ã—ã¦+9h
            if dt.tzinfo:
                dt = dt.astimezone(tz=None)  # ãƒ­ãƒ¼ã‚«ãƒ«tzï¼ˆGitHub Actionsã¯UTCï¼‰
                dt = dt + timedelta(hours=9)  # JST
            else:
                dt = dt + timedelta(hours=9)
            return fmt(dt)
    except Exception:
        pass
    return "å–å¾—ä¸å¯"


def chrome_driver():
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


# ========= ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ =========
def get_google_news(keyword: str):
    """
    Googleãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ (news.google.com) ã‚’Selenium+BS4ã§å–å¾—
    """
    driver = chrome_driver()
    url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    driver.get(url)
    time.sleep(4)

    # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«æ•°å›
    for _ in range(3):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data = []
    # è¨˜äº‹ã‚«ãƒ¼ãƒ‰ã¯articleã‚¿ã‚°ã€‚ã‚¯ãƒ©ã‚¹ã¯æºã‚Œã‚‹ãŸã‚ã€ã‚»ãƒ¬ã‚¯ã‚¿ã¯ä¿å®ˆçš„ã«
    for art in soup.find_all("article"):
        try:
            a = art.select_one("a.JtKRv, a.WwrzSb")
            t = art.select_one("time[datetime]")
            src = art.select_one("div.vr1PYe, div.SVJrMe")
            if not a or not t:
                continue
            title = a.get_text(strip=True)
            href = a.get("href")
            url = "https://news.google.com" + href[1:] if href and href.startswith("./") else href
            dt = datetime.strptime(t.get("datetime"), "%Y-%m-%dT%H:%M:%SZ") + timedelta(hours=9)
            pub = fmt(dt)
            source = src.get_text(strip=True) if src else "Google"
            if title and url:
                data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": pub, "å¼•ç”¨å…ƒ": source})
        except Exception:
            continue

    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(data)} ä»¶")
    return data


def get_yahoo_news(keyword: str):
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã‚’Selenium+BS4ã§å–å¾—
    """
    driver = chrome_driver()
    url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(url)
    time.sleep(4)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data = []
    # ãƒªã‚¹ãƒˆè¦ç´ ã¯ã‚¯ãƒ©ã‚¹ãŒé »ç¹ã«å¤‰ã‚ã‚‹ãŸã‚ã€ã‚†ã‚‹ã‚ã«æŠ½å‡º
    articles = soup.find_all("li")
    for li in articles:
        try:
            a = li.find("a", href=True)
            if not a:
                continue
            title = a.get_text(strip=True)
            url = a["href"]

            # æŠ•ç¨¿æ—¥è¡¨ç¤ºï¼ˆtimeã‚¿ã‚°ç­‰ï¼‰
            time_tag = li.find("time")
            date_str = time_tag.get_text(strip=True) if time_tag else ""
            # (ç«) ç­‰ã®æ›œæ—¥ã‚«ãƒƒã‚³æ¶ˆã—
            date_str = re.sub(r"\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)", "", date_str).strip()
            pub = "å–å¾—ä¸å¯"
            # ä»£è¡¨çš„ãª "YYYY/MM/DD HH:MM" ã«å¯¾å¿œ
            if re.match(r"\d{4}/\d{1,2}/\d{1,2}", date_str):
                try:
                    dt = try_parse_jst(date_str)
                    if dt:
                        pub = fmt(dt)
                except Exception:
                    pass

            # å‡ºå…¸ï¼ˆçŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’æ¨æ¸¬
            source = ""
            for tag in li.find_all(["span", "div"], string=True):
                text = tag.get_text(strip=True)
                if 2 <= len(text) <= 20 and not text.isdigit() and re.search(r"[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥A-Za-z]", text):
                    source = text
                    break
            if not source:
                source = "Yahoo"

            if title and url:
                data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": pub, "å¼•ç”¨å…ƒ": source})
        except Exception:
            continue

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(data)} ä»¶")
    return data


def get_msn_news(keyword: str):
    """
    MSNï¼ˆBingãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ï¼‰ã®ã‚«ãƒ¼ãƒ‰ã‚’Selenium+BS4ã§å–å¾—
    """
    base = jst_now()
    driver = chrome_driver()
    # æ–°ã—ã„é †æŒ‡å®šï¼ˆsortbydate=1ï¼‰
    url = f"https://www.bing.com/news/search?q={keyword}&qft=sortbydate%3d'1'&form=YFNR"
    driver.get(url)
    time.sleep(4)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    data = []
    cards = soup.select("div.news-card, news-card")
    for c in cards:
        try:
            title = (c.get("data-title") or "").strip()
            link = (c.get("data-url") or "").strip()
            author = (c.get("data-author") or "").strip()

            # ç›¸å¯¾æ™‚é–“ï¼ˆaria-labelç­‰ï¼‰
            pub_label = ""
            span = c.find("span", attrs={"aria-label": True})
            if span and span.has_attr("aria-label"):
                pub_label = span["aria-label"].strip()

            pub = parse_relative_time(pub_label, base)
            if pub == "å–å¾—ä¸å¯" and link:
                # æœ€çµ‚æ‰‹æ®µï¼šHEAD ã® Last-Modified
                pub = get_last_modified_datetime(link)

            if title and link:
                data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": link, "æŠ•ç¨¿æ—¥": pub, "å¼•ç”¨å…ƒ": author or "MSN"})
        except Exception:
            continue

    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(data)} ä»¶")
    return data


# ========= ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ I/O =========
def get_gspread_client():
    """
    - ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ãŒã‚ã‚Œã° dict ã‹ã‚‰èªè¨¼
    - ãªã‘ã‚Œã° credentials.json ã‚’ä½¿ç”¨
    """
    key = os.environ.get("GCP_SERVICE_ACCOUNT_KEY")
    if key:
        try:
            creds = json.loads(key)
            return gspread.service_account_from_dict(creds)
        except Exception as e:
            raise RuntimeError(f"GCP_SERVICE_ACCOUNT_KEY ã®JSONãŒä¸æ­£ã§ã™: {e}")
    # ãƒ­ãƒ¼ã‚«ãƒ«ç­‰
    return gspread.service_account(filename="credentials.json")


def append_to_source_sheet(sh, sheet_name: str, articles: list):
    """
    å„ã‚½ãƒ¼ã‚¹ã‚·ãƒ¼ãƒˆã¸URLå»é‡ã§è¿½è¨˜
    ã‚«ãƒ©ãƒ : ã‚¿ã‚¤ãƒˆãƒ« / URL / æŠ•ç¨¿æ—¥ / å¼•ç”¨å…ƒ
    """
    if not articles:
        print(f"âš ï¸ {sheet_name}: æ–°è¦0ä»¶")
        return

    try:
        ws = sh.worksheet(sheet_name)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=sheet_name, rows="1", cols="4")
        ws.append_row(["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])

    values = ws.get_all_values()
    existing_urls = set(row[1] for row in values[1:] if len(row) > 1)

    new_rows = []
    for a in articles:
        url = a.get("URL")
        if url and url not in existing_urls:
            new_rows.append([a.get("ã‚¿ã‚¤ãƒˆãƒ«", ""), url, a.get("æŠ•ç¨¿æ—¥", ""), a.get("å¼•ç”¨å…ƒ", sheet_name)])

    if new_rows:
        ws.append_rows(new_rows, value_input_option="USER_ENTERED")
        print(f"âœ… {sheet_name}: {len(new_rows)} ä»¶ è¿½è¨˜")
    else:
        print(f"âš ï¸ {sheet_name}: è¿½è¨˜å¯¾è±¡ãªã—ï¼ˆå…¨ã¦æ—¢å­˜URLï¼‰")


def compute_window(now_jst: datetime):
    """
    ç›´è¿‘å®Œäº†ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’è¿”ã™:
    - å®Ÿè¡ŒãŒ 15:00 ä»¥é™:   æ˜¨æ—¥15:00 ã€œ ä»Šæ—¥14:59:59 / ã‚·ãƒ¼ãƒˆå=ä»Šæ—¥ã®YYMMDD
    - å®Ÿè¡ŒãŒ 15:00 ã‚ˆã‚Šå‰: ä¸€æ˜¨æ—¥15:00 ã€œ æ˜¨æ—¥14:59:59 / ã‚·ãƒ¼ãƒˆå=æ˜¨æ—¥ã®YYMMDD
    """
    today = now_jst.date()
    fifteen = datetime.combine(today, dtime(hour=15, minute=0))
    if now_jst >= fifteen:
        start = fifteen - timedelta(days=1)
        end = fifteen - timedelta(seconds=1)
        label = today.strftime("%y%m%d")
    else:
        start = fifteen - timedelta(days=2)
        end = fifteen - timedelta(days=1, seconds=1)
        label = (today - timedelta(days=1)).strftime("%y%m%d")
    return start, end, label


def build_daily_sheet(sh):
    """
    å„ã‚½ãƒ¼ã‚¹ã‚·ãƒ¼ãƒˆã‹ã‚‰ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®è¨˜äº‹ã‚’é›†ã‚ã€
    ã‚·ãƒ¼ãƒˆå YYMMDD ã§ä¸€è¦§åŒ–
    ä¸¦ã³: MSN â†’ Google â†’ Yahooï¼ˆå„ã‚½ãƒ¼ã‚¹å†…ã¯æŠ•ç¨¿æ—¥é™é †ï¼‰
    ã‚«ãƒ©ãƒ : ã‚½ãƒ¼ã‚¹ / URL / ã‚¿ã‚¤ãƒˆãƒ« / æŠ•ç¨¿æ—¥ / å¼•ç”¨å…ƒ
    """
    now = jst_now()
    start, end, label = compute_window(now)
    print(f"ğŸ•’ é›†ç´„æœŸé–“: {fmt(start)} ã€œ {fmt(end)} â†’ ã‚·ãƒ¼ãƒˆå: {label}")

    rows_by_source = {"MSN": [], "Google": [], "Yahoo": []}

    for src in ["MSN", "Google", "Yahoo"]:
        try:
            ws = sh.worksheet(src)
        except gspread.exceptions.WorksheetNotFound:
            print(f"âš ï¸ {src} ã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            continue

        for d in ws.get_all_records():
            url = d.get("URL") or ""
            title = d.get("ã‚¿ã‚¤ãƒˆãƒ«") or ""
            posted = try_parse_jst(d.get("æŠ•ç¨¿æ—¥") or "")
            origin = d.get("å¼•ç”¨å…ƒ") or src
            if not url or not title or not posted:
                continue
            if start <= posted <= end:
                rows_by_source[src].append([src, url, title, fmt(posted), origin])

    # å‡ºåŠ›é †: MSN â†’ Google â†’ Yahoo
    ordered_rows = []
    for src in ["MSN", "Google", "Yahoo"]:
        # ã‚½ãƒ¼ã‚¹å†…URLå»é‡
        seen = set()
        uniq = []
        for r in rows_by_source[src]:
            if r[1] not in seen:
                seen.add(r[1])
                uniq.append(r)
        # æŠ•ç¨¿æ—¥é™é †
        uniq.sort(key=lambda x: try_parse_jst(x[3]) or datetime(1970, 1, 1), reverse=True)
        ordered_rows.extend(uniq)

    headers = ["ã‚½ãƒ¼ã‚¹", "URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"]
    try:
        out_ws = sh.worksheet(label)
        out_ws.clear()
        out_ws.append_row(headers)
    except gspread.exceptions.WorksheetNotFound:
        out_ws = sh.add_worksheet(title=label, rows=str(max(2, len(ordered_rows) + 5)), cols="5")
        out_ws.append_row(headers)

    if ordered_rows:
        out_ws.append_rows(ordered_rows, value_input_option="USER_ENTERED")
        print(f"âœ… é›†ç´„ã‚·ãƒ¼ãƒˆ {label}: {len(ordered_rows)} ä»¶ å‡ºåŠ›")
    else:
        print(f"âš ï¸ é›†ç´„ã‚·ãƒ¼ãƒˆ {label}: å¯¾è±¡è¨˜äº‹ãªã—")


# ========= ãƒ¡ã‚¤ãƒ³ =========
def main():
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {KEYWORD}")
    print(f"ğŸ“„ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ: {SPREADSHEET_ID}")

    gc = get_gspread_client()
    sh = gc.open_by_key(SPREADSHEET_ID)

    print("\n--- Google News ---")
    google_items = get_google_news(KEYWORD)
    append_to_source_sheet(sh, "Google", google_items)

    print("\n--- Yahoo! News ---")
    yahoo_items = get_yahoo_news(KEYWORD)
    append_to_source_sheet(sh, "Yahoo", yahoo_items)

    print("\n--- MSN News ---")
    msn_items = get_msn_news(KEYWORD)
    append_to_source_sheet(sh, "MSN", msn_items)

    print("\n--- æ—¥æ¬¡é›†ç´„ï¼ˆMSNâ†’Googleâ†’Yahoo / Aåˆ—=ã‚½ãƒ¼ã‚¹ï¼‰ ---")
    build_daily_sheet(sh)


if __name__ == "__main__":
    main()
