import os
import re
import json
import time
import requests
import datetime
import gspread
from bs4 import BeautifulSoup
from google.oauth2.service_account import Credentials
from openai import OpenAI

# ========= è¨­å®š ==========
NEWS_KEYWORD = os.getenv("NEWS_KEYWORD", "æ—¥ç”£")
SPREADSHEET_ID = os.getenv("SPREADSHEET_ID")
GCP_SERVICE_ACCOUNT_KEY = os.getenv("GCP_SERVICE_ACCOUNT_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# ========= Google Sheets èªè¨¼ ==========
def get_gspread_client():
    keyfile_dict = json.loads(GCP_SERVICE_ACCOUNT_KEY)
    creds = Credentials.from_service_account_info(
        keyfile_dict,
        scopes=["https://www.googleapis.com/auth/spreadsheets",
                "https://www.googleapis.com/auth/drive"]
    )
    return gspread.authorize(creds)

# ========= Yahooã‚³ãƒ¡ãƒ³ãƒˆæ•° ==========
def get_yahoo_comment_count(url: str) -> int:
    try:
        if "news.yahoo.co.jp" not in url:
            return 0
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        html = res.text
        m = re.search(r'ã‚³ãƒ¡ãƒ³ãƒˆæ•°.*?([0-9,]+)', html)
        if m:
            return int(m.group(1).replace(",", ""))
        # äºˆå‚™: data å±æ€§ã«å«ã¾ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        m = re.search(r'"commentCount":\s*([0-9]+)', html)
        if m:
            return int(m.group(1))
    except Exception:
        return 0
    return 0

# ========= ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾— ==========
def fetch_google_news(keyword):
    url = f"https://news.google.com/rss/search?q={keyword}+when:1d&hl=ja&gl=JP&ceid=JP:ja"
    res = requests.get(url)
    soup = BeautifulSoup(res.content, "xml")
    items = []
    for item in soup.find_all("item"):
        title = item.title.text
        link = item.link.text
        pub_date = item.pubDate.text if item.pubDate else ""
        source = item.source.text if item.source else ""
        # pubDate ã‚’æ—¥ä»˜æ•´å½¢
        try:
            dt = datetime.datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
            pub_date = dt.strftime("%Y/%m/%d %H:%M")
        except Exception:
            pass
        items.append({
            "source": "Google",
            "url": link,
            "title": title,
            "date": pub_date,
            "origin": source,
            "comment": 0
        })
    return items

def fetch_yahoo_news(keyword):
    url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    items = []
    for a in soup.select("a.newsFeed_item_link"):
        title = a.get("title")
        link = a.get("href")
        if not title or not link:
            continue
        date = ""
        origin = "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹"
        # ã‚³ãƒ¡ãƒ³ãƒˆæ•°
        comment_count = get_yahoo_comment_count(link)
        items.append({
            "source": "Yahoo",
            "url": link,
            "title": title,
            "date": date,
            "origin": origin,
            "comment": comment_count
        })
    return items

def fetch_msn_news(keyword):
    url = f"https://www.bing.com/news/search?q={keyword}&qft=interval%3d%227%22&form=YFNR"
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    items = []
    for a in soup.select("a.title"):
        title = a.text.strip()
        link = a.get("href")
        if not title or not link:
            continue
        date = ""
        origin = "MSN"
        items.append({
            "source": "MSN",
            "url": link,
            "title": title,
            "date": date,
            "origin": origin,
            "comment": 0
        })
    return items

# ========= Gemini åˆ†æ ==========
def analyze_titles_gemini(titles):
    client = OpenAI(api_key=GEMINI_API_KEY, base_url="https://generativelanguage.googleapis.com/v1beta/openai/")
    prompt = """ã‚ãªãŸã¯æ•è…•é›‘èªŒè¨˜è€…ã§ã™ã€‚ ä»¥ä¸‹ã®Webãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã«ã¤ã„ã¦ã€
â‘ ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ã‚’åˆ¤åˆ¥
â‘¡è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’åˆ¤åˆ¥ï¼ˆä¼šç¤¾ãƒ»è»Šãƒ»æŠ€è¡“ï¼ˆEV/e-POWER/e-4ORCE/AD/ADAS/ãã®ä»–ï¼‰ãƒ»ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚¹ãƒãƒ¼ãƒ„ãƒ»æ ªå¼ãƒ»æ”¿æ²»ãƒ»çµŒæ¸ˆãƒ»ã‚¹ãƒãƒ¼ãƒ„ãƒ»ãã®ä»–ï¼‰
å‡ºåŠ›ã¯ JSON é…åˆ—ã§ã€å„è¦ç´ ãŒ {"title": "...", "sentiment": "...", "category": "..."} ã¨ã„ã†å½¢å¼ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚
ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§:
""" + "\n".join([f"- {t}" for t in titles])
    resp = client.chat.completions.create(
        model="gemini-1.5-flash",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )
    text = resp.choices[0].message.content
    try:
        data = json.loads(text)
        return {d["title"]: (d["sentiment"], d["category"]) for d in data}
    except Exception:
        return {}

# ========= é›†ç´„ã—ã¦ã‚·ãƒ¼ãƒˆã«å‡ºåŠ› ==========
def build_daily_sheet(sh, items):
    today = datetime.datetime.now()
    start = (today - datetime.timedelta(days=1)).replace(hour=15, minute=0, second=0, microsecond=0)
    end = today.replace(hour=14, minute=59, second=0, microsecond=0)
    sheet_name = today.strftime("%y%m%d")

    filtered = {"MSN": [], "Google": [], "Yahoo": []}
    no_date = 0
    for it in items:
        try:
            if it["date"]:
                dt = datetime.datetime.strptime(it["date"], "%Y/%m/%d %H:%M")
                if start <= dt <= end:
                    filtered[it["source"]].append(it)
            else:
                no_date += 1
        except Exception:
            no_date += 1

    # Gemini åˆ†æ
    titles = [it["title"] for group in filtered.values() for it in group]
    results = analyze_titles_gemini(titles)

    try:
        ws = sh.worksheet(sheet_name)
        sh.del_worksheet(ws)
    except Exception:
        pass
    ws = sh.add_worksheet(title=sheet_name, rows="1000", cols="8")

    headers = ["ã‚½ãƒ¼ã‚¹", "URL", "ã‚¿ã‚¤ãƒˆãƒ«", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª"]
    ws.update("A1:H1", [headers])

    rows = []
    for src in ["MSN", "Google", "Yahoo"]:
        for it in filtered[src]:
            sentiment, category = results.get(it["title"], ("", ""))
            rows.append([
                it["source"], it["url"], it["title"], it["date"],
                it["origin"], it["comment"], sentiment, category
            ])

    if rows:
        ws.update("A2", rows)

    print(f"ğŸ•’ é›†ç´„æœŸé–“: {start.strftime('%Y/%m/%d %H:%M')} ã€œ {end.strftime('%Y/%m/%d %H:%M')} â†’ ã‚·ãƒ¼ãƒˆå: {sheet_name}")
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿çµæœ: MSN={len(filtered['MSN'])}, Google={len(filtered['Google'])}, Yahoo={len(filtered['Yahoo'])}, æ—¥ä»˜ç„¡ã—ã‚¹ã‚­ãƒƒãƒ—={no_date}")
    print(f"âœ… é›†ç´„ã‚·ãƒ¼ãƒˆ {sheet_name}: {len(rows)} ä»¶")
    return sheet_name

# ========= Main ==========
def main():
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {NEWS_KEYWORD}")
    print(f"ğŸ“„ SPREADSHEET_ID: {SPREADSHEET_ID}")
    gc = get_gspread_client()
    sh = gc.open_by_key(SPREADSHEET_ID)
    print(f"ğŸ“˜ Opened spreadsheet title: {sh.title}")

    google_items = fetch_google_news(NEWS_KEYWORD)
    yahoo_items = fetch_yahoo_news(NEWS_KEYWORD)
    msn_items = fetch_msn_news(NEWS_KEYWORD)

    print(f"--- å–å¾— ---")
    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(google_items)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾— {sum(1 for i in google_items if i['date'])} ä»¶ï¼‰")
    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(yahoo_items)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾— {sum(1 for i in yahoo_items if i['date'])} ä»¶ï¼‰")
    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(msn_items)} ä»¶ï¼ˆæŠ•ç¨¿æ—¥å–å¾—/æ¨å®š {sum(1 for i in msn_items if i['date'])} ä»¶ï¼‰")

    all_items = google_items + yahoo_items + msn_items
    build_daily_sheet(sh, all_items)

if __name__ == "__main__":
    main()
